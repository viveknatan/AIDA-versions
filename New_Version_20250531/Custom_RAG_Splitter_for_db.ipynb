{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pymupdf  # PyMuPDF for PDF processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TableChunk:\n",
    "    \"\"\"Represents a database table chunk with metadata\"\"\"\n",
    "    table_name: str\n",
    "    content: str\n",
    "    page_numbers: List[int]\n",
    "    section_type: str  # 'table_description', 'overview', 'relationships'\n",
    "\n",
    "class NorthwindSchemaChunker:\n",
    "    \"\"\"\n",
    "    Splits the Northwind Database Schema PDF into semantic chunks\n",
    "    focused on individual database tables for RAG applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Patterns to identify table sections\n",
    "        self.table_patterns = {\n",
    "            'table_header': re.compile(r'^([A-Z][a-zA-Z_]+)\\s*$', re.MULTILINE),\n",
    "            'description_start': re.compile(r'Description:\\s*', re.IGNORECASE),\n",
    "            'columns_start': re.compile(r'Columns:\\s*', re.IGNORECASE),\n",
    "            'primary_key': re.compile(r'Primary Key:\\s*', re.IGNORECASE),\n",
    "            'foreign_keys': re.compile(r'Foreign Keys:\\s*', re.IGNORECASE),\n",
    "            'relationships': re.compile(r'Relationships:\\s*', re.IGNORECASE)\n",
    "        }\n",
    "        \n",
    "        # Known table names from the schema\n",
    "        self.known_tables = {\n",
    "            'Orders', 'Order_Details', 'Customers', 'Employees', 'Products',\n",
    "            'Categories', 'Suppliers', 'Shippers', 'Regions', 'Territories',\n",
    "            'EmployeeTerritories', 'CustomerDemographics', 'CustomerCustomerDemo',\n",
    "            'US_States'\n",
    "        }\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"Extract text from PDF with page information\"\"\"\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        pages_content = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            pages_content.append({\n",
    "                'page_number': page_num + 1,\n",
    "                'content': text\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        return pages_content\n",
    "    \n",
    "    def identify_table_sections(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Identify and extract table sections from the text\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Split text into potential sections based on table names\n",
    "        lines = text.split('\\n')\n",
    "        current_section = []\n",
    "        current_table = None\n",
    "        in_table_section = False\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check if this line is a table header\n",
    "            if line in self.known_tables and (\n",
    "                i == 0 or \n",
    "                lines[i-1].strip() == '' or \n",
    "                any(keyword in lines[i-1] for keyword in ['relationships', 'table.', 'table)'])\n",
    "            ):\n",
    "                # Save previous section if it exists\n",
    "                if current_section and current_table:\n",
    "                    sections.append({\n",
    "                        'table_name': current_table,\n",
    "                        'content': '\\n'.join(current_section).strip()\n",
    "                    })\n",
    "                \n",
    "                # Start new section\n",
    "                current_table = line\n",
    "                current_section = [line]\n",
    "                in_table_section = True\n",
    "                \n",
    "            elif in_table_section:\n",
    "                current_section.append(line)\n",
    "                \n",
    "                # Check if we've reached the end of this table section\n",
    "                # (next table name or specific end markers)\n",
    "                next_lines = lines[i+1:i+3] if i+1 < len(lines) else []\n",
    "                if any(next_line.strip() in self.known_tables for next_line in next_lines):\n",
    "                    # We're about to hit another table\n",
    "                    continue\n",
    "        \n",
    "        # Don't forget the last section\n",
    "        if current_section and current_table:\n",
    "            sections.append({\n",
    "                'table_name': current_table,\n",
    "                'content': '\\n'.join(current_section).strip()\n",
    "            })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def create_overview_chunk(self, pages_content: List[Dict]) -> Document:\n",
    "        \"\"\"Create an overview chunk from the first page\"\"\"\n",
    "        overview_text = \"\"\n",
    "        overview_pages = []\n",
    "        \n",
    "        for page in pages_content[:2]:  # First two pages typically contain overview\n",
    "            if any(keyword in page['content'].lower() for keyword in \n",
    "                   ['overview', 'entity-relationship', 'northwind database schema']):\n",
    "                overview_text += page['content'] + \"\\n\\n\"\n",
    "                overview_pages.append(page['page_number'])\n",
    "        \n",
    "        # Clean up the overview text\n",
    "        overview_text = re.sub(r'\\n{3,}', '\\n\\n', overview_text)\n",
    "        \n",
    "        return Document(\n",
    "            page_content=overview_text.strip(),\n",
    "            metadata={\n",
    "                'section_type': 'overview',\n",
    "                'table_name': 'schema_overview',\n",
    "                'pages': overview_pages,\n",
    "                'chunk_type': 'database_schema_overview'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def create_table_chunks(self, pages_content: List[Dict]) -> List[Document]:\n",
    "        \"\"\"Create individual chunks for each database table\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Combine all text to process as one document\n",
    "        full_text = \"\"\n",
    "        page_mapping = {}\n",
    "        char_count = 0\n",
    "        \n",
    "        for page in pages_content:\n",
    "            page_start = char_count\n",
    "            page_text = page['content']\n",
    "            full_text += page_text + \"\\n\\n\"\n",
    "            char_count += len(page_text) + 2\n",
    "            page_mapping[page['page_number']] = (page_start, char_count)\n",
    "        \n",
    "        # Extract table sections using improved pattern matching\n",
    "        table_sections = self.extract_table_sections_advanced(full_text, pages_content)\n",
    "        \n",
    "        for section in table_sections:\n",
    "            # Clean and format the content\n",
    "            content = self.clean_table_content(section['content'])\n",
    "            \n",
    "            # Determine which pages this content spans\n",
    "            section_pages = self.find_content_pages(section['content'], pages_content)\n",
    "            \n",
    "            chunk = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'section_type': 'table_description',\n",
    "                    'table_name': section['table_name'].lower(),\n",
    "                    'pages': section_pages,\n",
    "                    'chunk_type': 'database_table',\n",
    "                    'table_name_display': section['table_name']\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def extract_table_sections_advanced(self, full_text: str, pages_content: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Advanced extraction of table sections with better boundary detection\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Pattern to find table headers followed by descriptions\n",
    "        table_pattern = re.compile(\n",
    "            r'\\n(' + '|'.join(self.known_tables) + r')\\s*\\n.*?Description:\\s*(.*?)(?=\\n(?:' + \n",
    "            '|'.join(self.known_tables) + r')\\s*\\n|\\nSummary of Table Relationships|\\Z)',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        matches = table_pattern.finditer(full_text)\n",
    "        \n",
    "        for match in matches:\n",
    "            table_name = match.group(1)\n",
    "            full_section = match.group(0)\n",
    "            \n",
    "            sections.append({\n",
    "                'table_name': table_name,\n",
    "                'content': full_section.strip()\n",
    "            })\n",
    "        \n",
    "        # If regex approach doesn't work well, fall back to line-by-line parsing\n",
    "        if not sections:\n",
    "            sections = self.identify_table_sections(full_text)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def clean_table_content(self, content: str) -> str:\n",
    "        \"\"\"Clean and format table content for better RAG performance\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "        content = re.sub(r' {2,}', ' ', content)\n",
    "        \n",
    "        # Ensure proper formatting for key sections\n",
    "        content = re.sub(r'(Description:|Columns:|Primary Key:|Foreign Keys:|Relationships:)', \n",
    "                        r'\\n\\1', content)\n",
    "        \n",
    "        # Clean up bullet points and formatting\n",
    "        content = re.sub(r'^\\s*[•·]\\s*', '- ', content, flags=re.MULTILINE)\n",
    "        \n",
    "        return content.strip()\n",
    "    \n",
    "    def find_content_pages(self, content: str, pages_content: List[Dict]) -> List[int]:\n",
    "        \"\"\"Find which pages contain the given content\"\"\"\n",
    "        pages = []\n",
    "        content_words = set(content.lower().split()[:20])  # First 20 words for matching\n",
    "        \n",
    "        for page in pages_content:\n",
    "            page_words = set(page['content'].lower().split())\n",
    "            # If significant overlap, this page contains part of the content\n",
    "            if len(content_words.intersection(page_words)) > len(content_words) * 0.3:\n",
    "                pages.append(page['page_number'])\n",
    "        \n",
    "        return pages\n",
    "    \n",
    "    def create_relationships_chunk(self, pages_content: List[Dict]) -> Optional[Document]:\n",
    "        \"\"\"Create a chunk for the relationships summary section\"\"\"\n",
    "        for page in pages_content:\n",
    "            if 'Summary of Table Relationships' in page['content']:\n",
    "                # Extract the relationships section\n",
    "                content = page['content']\n",
    "                start_idx = content.find('Summary of Table Relationships')\n",
    "                if start_idx != -1:\n",
    "                    relationships_content = content[start_idx:]\n",
    "                    \n",
    "                    return Document(\n",
    "                        page_content=relationships_content,\n",
    "                        metadata={\n",
    "                            'section_type': 'relationships_summary',\n",
    "                            'table_name': 'all_tables',\n",
    "                            'pages': [page['page_number']],\n",
    "                            'chunk_type': 'database_relationships'\n",
    "                        }\n",
    "                    )\n",
    "        return None\n",
    "    \n",
    "    def chunk_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Main method to chunk the PDF into table-focused documents\"\"\"\n",
    "        # Extract text from PDF\n",
    "        pages_content = self.extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        # Create overview chunk\n",
    "        overview_chunk = self.create_overview_chunk(pages_content)\n",
    "        chunks.append(overview_chunk)\n",
    "        \n",
    "        # Create individual table chunks\n",
    "        table_chunks = self.create_table_chunks(pages_content)\n",
    "        chunks.extend(table_chunks)\n",
    "        \n",
    "        # Create relationships summary chunk\n",
    "        relationships_chunk = self.create_relationships_chunk(pages_content)\n",
    "        if relationships_chunk:\n",
    "            chunks.append(relationships_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Usage example and helper functions\n",
    "def prepare_chunks_for_vector_store(chunks: List[Document], \n",
    "                                   max_chunk_size: int = 2000) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Prepare chunks for vector store by ensuring they're not too large\n",
    "    and have consistent metadata\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    final_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) <= max_chunk_size:\n",
    "            final_chunks.append(chunk)\n",
    "        else:\n",
    "            # Split large chunks while preserving metadata\n",
    "            sub_chunks = text_splitter.split_documents([chunk])\n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                sub_chunk.metadata.update(chunk.metadata)\n",
    "                sub_chunk.metadata['sub_chunk'] = i\n",
    "                final_chunks.append(sub_chunk)\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "def main(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Main function to process the Northwind schema PDF\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the Northwind schema PDF file\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects ready for vector database ingestion\n",
    "    \"\"\"\n",
    "    chunker = NorthwindSchemaChunker()\n",
    "    \n",
    "    # Extract table-focused chunks\n",
    "    chunks = chunker.chunk_pdf(pdf_path)\n",
    "    \n",
    "    # Prepare for vector store (ensure reasonable chunk sizes)\n",
    "    final_chunks = prepare_chunks_for_vector_store(chunks)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Created {len(final_chunks)} chunks:\")\n",
    "    for chunk in final_chunks:\n",
    "        print(f\"- {chunk.metadata['table_name']} ({chunk.metadata['section_type']}) \"\n",
    "              f\"- {len(chunk.page_content)} chars\")\n",
    "    \n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage\n",
    "chunker = NorthwindSchemaChunker()\n",
    "chunks = chunker.chunk_pdf(\"data/Northwind_Traders_Database_Overview.pdf\")\n",
    "\n",
    "# Prepare for vector store\n",
    "final_chunks = prepare_chunks_for_vector_store(chunks, max_chunk_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
