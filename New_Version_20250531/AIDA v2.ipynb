{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langgraph==0.2.14 langchain==0.2.14 langchain_openai==0.1.23 langchain_core==0.2.35 langchain-community\n",
    "\n",
    "#!pip install -qU --disable-pip-version-check qdrant-client pymupdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "path =\"data/\"\n",
    "loader = DirectoryLoader(path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Advanced PDF Section-Based Chunking\n",
    "\n",
    "Let's replace the basic text splitter with our advanced PDF section-based chunker that preserves document structure and uses tiktoken for accurate token counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Available Tiktoken Encodings:\n",
      "   cl100k_base: GPT-4, GPT-3.5-turbo, text-embedding-ada-002\n",
      "   p50k_base: text-davinci-002, text-davinci-003\n",
      "   r50k_base: GPT-3 models (davinci, curie, babbage, ada)\n",
      "   gpt2: GPT-2 models\n"
     ]
    }
   ],
   "source": [
    "# Import our custom PDF section chunker\n",
    "from pdf_section_chunker import PDFSectionChunker, chunk_northwind_pdf, get_encoding_info, TIKTOKEN_AVAILABLE\n",
    "\n",
    "# Show available encodings\n",
    "if TIKTOKEN_AVAILABLE:\n",
    "    print(\"🔢 Available Tiktoken Encodings:\")\n",
    "    for encoding, models in get_encoding_info().items():\n",
    "        print(f\"   {encoding}: {models}\")\n",
    "else:\n",
    "    print(\"⚠️ Tiktoken not available - using character-based chunking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing Northwind_Traders_Database_Overview.pdf with section-based chunking...\n",
      "✓ Using token-based chunking with cl100k_base encoding\n",
      "✅ Created 34 intelligent chunks from the PDF\n",
      "\n",
      "📊 Chunk Statistics:\n",
      "   Character count - Min: 159, Max: 2049, Avg: 1257\n",
      "   Token count - Min: 34, Max: 396, Avg: 248\n",
      "   Chunking method: tokens\n",
      "   Encoding: cl100k_base\n",
      "\n",
      "📋 Sample Chunks:\n",
      "\n",
      "   Chunk 1: Northwind Traders Database Overview\n",
      "   Page 1 | Level 1 | 427 chars | 77 tokens\n",
      "   Preview: Northwind Traders is a fictional wholesale food import-export company, and the Northwind database models its day-to-day business operations. This samp...\n",
      "\n",
      "   Chunk 2: ERP\n",
      "   Page 1 | Level 4 | 534 chars | 88 tokens\n",
      "   Preview: (Enterprise Resource Planning) schema for a sales company . The database captures all sales transactions between Northwind and its customers, as well ...\n",
      "\n",
      "   Chunk 3: Core Business Processes Represented\n",
      "   Page 1 | Level 1 | 407 chars | 67 tokens\n",
      "   Preview: Northwind’s schema is designed to support several core business processes for the company. These include managing customer information, processing and...\n"
     ]
    }
   ],
   "source": [
    "# Process the Northwind PDF with intelligent section-based chunking\n",
    "print(\"🔄 Processing Northwind_Traders_Database_Overview.pdf with section-based chunking...\")\n",
    "\n",
    "# Use our advanced chunker with token optimization for embedding models\n",
    "chunks = chunk_northwind_pdf(\n",
    "    pdf_path=\"data/Northwind_Traders_Database_Overview.pdf\",\n",
    "    use_tokens=True,\n",
    "    encoding_name=\"cl100k_base\"  # Optimal for OpenAI models\n",
    ")\n",
    "\n",
    "print(f\"✅ Created {len(chunks)} intelligent chunks from the PDF\")\n",
    "\n",
    "# Display statistics\n",
    "if chunks:\n",
    "    char_counts = [chunk.metadata.get('char_count', len(chunk.content)) for chunk in chunks]\n",
    "    token_counts = [chunk.metadata.get('token_count', 0) for chunk in chunks]\n",
    "    \n",
    "    print(f\"\\n📊 Chunk Statistics:\")\n",
    "    print(f\"   Character count - Min: {min(char_counts)}, Max: {max(char_counts)}, Avg: {sum(char_counts)//len(char_counts)}\")\n",
    "    \n",
    "    if any(token_counts):\n",
    "        print(f\"   Token count - Min: {min(token_counts)}, Max: {max(token_counts)}, Avg: {sum(token_counts)//len(token_counts)}\")\n",
    "        print(f\"   Chunking method: {chunks[0].metadata.get('chunking_method', 'unknown')}\")\n",
    "        print(f\"   Encoding: {chunks[0].metadata.get('encoding', 'N/A')}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\n📋 Sample Chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    char_count = chunk.metadata.get('char_count', len(chunk.content))\n",
    "    token_count = chunk.metadata.get('token_count', 'N/A')\n",
    "    \n",
    "    print(f\"\\n   Chunk {i+1}: {chunk.title}\")\n",
    "    print(f\"   Page {chunk.page_number} | Level {chunk.section_level} | {char_count} chars | {token_count} tokens\")\n",
    "    preview = chunk.content[:150] + \"...\" if len(chunk.content) > 150 else chunk.content\n",
    "    print(f\"   Preview: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Converted 34 smart chunks to LangChain Document format\n",
      "📄 Sample metadata: {'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 1, 'section_title': 'Northwind Traders Database Overview', 'section_level': 1, 'char_count': 427, 'token_count': 77, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': False}\n"
     ]
    }
   ],
   "source": [
    "# Convert our DocumentChunk objects to LangChain Document format\n",
    "from langchain.schema import Document\n",
    "\n",
    "def convert_chunks_to_langchain_docs(chunks):\n",
    "    \"\"\"Convert our DocumentChunk objects to LangChain Document format.\"\"\"\n",
    "    langchain_docs = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Create metadata that includes all our enhanced information\n",
    "        metadata = {\n",
    "            \"source\": chunk.metadata.get('source', 'Northwind_Traders_Database_Overview.pdf'),\n",
    "            \"page\": chunk.page_number,\n",
    "            \"section_title\": chunk.title,\n",
    "            \"section_level\": chunk.section_level,\n",
    "            \"char_count\": chunk.metadata.get('char_count', len(chunk.content)),\n",
    "            \"token_count\": chunk.metadata.get('token_count', 0),\n",
    "            \"chunking_method\": chunk.metadata.get('chunking_method', 'unknown'),\n",
    "            \"encoding\": chunk.metadata.get('encoding', 'N/A'),\n",
    "            \"chunk_type\": chunk.metadata.get('chunk_type', 'section'),\n",
    "            \"is_split\": chunk.metadata.get('is_split', False)\n",
    "        }\n",
    "        \n",
    "        # Create LangChain Document\n",
    "        doc = Document(\n",
    "            page_content=chunk.content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        langchain_docs.append(doc)\n",
    "    \n",
    "    return langchain_docs\n",
    "\n",
    "# Convert our smart chunks to LangChain format\n",
    "smart_docs = convert_chunks_to_langchain_docs(chunks)\n",
    "\n",
    "print(f\"📚 Converted {len(smart_docs)} smart chunks to LangChain Document format\")\n",
    "print(f\"📄 Sample metadata: {smart_docs[0].metadata}\")\n",
    "\n",
    "# Replace the old split_chunks with our smart documents\n",
    "split_chunks = smart_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def tiktoken_len(text):\n",
    "#     tokens = tiktoken.encoding_for_model(\"gpt-4o-mini\").encode(\n",
    "#         text,\n",
    "#     )\n",
    "#     return len(tokens)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 200,\n",
    "#     chunk_overlap = 20,\n",
    "#     length_function = tiktoken_len,\n",
    "# )\n",
    "\n",
    "# split_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    split_chunks,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"northwind_data\",\n",
    ")\n",
    "\n",
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 4, 'section_title': 'Orders Table (Part 2)', 'section_level': 3, 'char_count': 435, 'token_count': 101, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '807d92635838448e8e344c940c894d79', '_collection_name': 'northwind_data'}, page_content='For example, an order record might show that Order #10248 was placed by Customer ALFKI on July 4, 2025 , entered by Employee #5 , and shipped via Speedy Express on July 9, 2025 with a freight charge of $32. Freight and shipper data can be used for logistics analysis (e.g. average shipping times or costs), while the employee and customer links support sales performance tracking (e.g. orders per employee, order history per customer).'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 9, 'section_title': 'Scenario 1: Processing a Customer Order (Part 1)', 'section_level': 3, 'char_count': 1658, 'token_count': 379, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '67e0697bbade4f3aaec02f1647023f34', '_collection_name': 'northwind_data'}, page_content='Acme Foods , a customer, places an order for 10 boxes of Chai Tea (a product). Here’s how that order is recorded and fulfilled using the database tables: An Orders record is created for the new order. This includes selecting Acme Foods’ CustomerID (linking to the Customers table to get all needed info like address), and assigning an EmployeeID for the sales representative who handles the order (linking to Employees). The OrderDate is set to 1. 9 today’s date, and a RequiredDate is set based on when Acme needs the goods. At this stage, ShippedDate is empty (not shipped yet) and a provisional ShipVia (shipper) might be chosen or left for later, with no Freight cost entered yet. Two Order Details rows are added (because the customer wants 10 boxes of Chai Tea and let’s say 5 boxes of Chang (another product) as well). Each Order Details entry uses the new Order’s OrderID, and one has ProductID for Chai Tea, quantity 10, the other has ProductID for Chang, quantity 5. The unit prices for each (at the time of order) are pulled from the Products table and recorded in Order Details. If there’s any discount (perhaps the customer has a bulk discount), that is noted in the Discount field. As soon as the order is saved, Northwind’s inventory can be updated. The system will subtract 10 units from Chai Tea ’s UnitsInStock in the Products table, and 5 units from Chang ’s UnitsInStock, to reflect that these items are allocated to this order. If any product’s stock fell below its ReorderLevel because of this order, it’s a trigger for the purchasing team to place a restock order with that product’s supplier. The order is then processed for shipping.'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 11, 'section_title': 'Scenario 3: Employee Territory Assignment and Regional Sales (Part 3)', 'section_level': 3, 'char_count': 159, 'token_count': 70, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': 'a4a8e6ee41bb4e438f7736004d2dc05a', '_collection_name': 'northwind_data'}, page_content='I`m going to work with Microsoft’s… | by Mythilyram | Medium https://medium.com/@mythilyrm/the-northwind-database-f2fa1f59daa7 • 2 3 1 2 3 6 7 4 12 5 8 9 10 11'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 1, 'section_title': 'Northwind Traders Database Overview', 'section_level': 1, 'char_count': 427, 'token_count': 77, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': False, '_id': '857176976cb241af9f1e159a6f7e42c2', '_collection_name': 'northwind_data'}, page_content='Northwind Traders is a fictional wholesale food import-export company, and the Northwind database models its day-to-day business operations. This sample relational database was originally created by Microsoft as a teaching example and includes all the information needed to run a small trading business . In Northwind, you’ll find data about customers, products, employees, suppliers, orders, and more – essentially a miniature')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_retriever.invoke(\"What day is it today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Any\n",
    "from pydantic import Field\n",
    "\n",
    "class ScoreFilteredRetriever(BaseRetriever):\n",
    "    vectorstore: Any = Field()\n",
    "    score_threshold: float = Field(default=0.5)\n",
    "    k: int = Field(default=5)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=self.k)\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_docs = [\n",
    "            doc for doc, score in docs_with_scores \n",
    "            if score >= self.score_threshold\n",
    "        ]\n",
    "        \n",
    "        return filtered_docs\n",
    "\n",
    "# Use it\n",
    "filtered_retriever = ScoreFilteredRetriever(\n",
    "    vectorstore=qdrant_vectorstore,  # Use keyword arguments\n",
    "    score_threshold=0.3,\n",
    "    k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 4, 'section_title': 'Orders Table (Part 1)', 'section_level': 3, 'char_count': 1815, 'token_count': 365, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': 'ebfd43b2e6084ddfbe75d9a5ae8c19d9', '_collection_name': 'northwind_data'}, page_content='The Orders table represents sales orders placed by customers. Each row in Orders is a single order, identified by a unique OrderID (primary key). The table captures both who and when for each sale, as well as how it is fulfilled. Key columns include: - CustomerID – which customer placed the order (foreign key to Customers table). - EmployeeID – which employee handled or entered the order (foreign key to Employees table, indicating the sales representative or order taker). - OrderDate – the date the order was placed. - RequiredDate – the date by which the customer requested the goods (often used for scheduling). - ShippedDate – the date the order was actually shipped out. Several other fields record shipping and fulfillment details: - ShipVia – a foreign key indicating which shipper/carrier was used to deliver the order (links to the Shippers table). - Freight – the shipping cost for the order (a numeric amount paid for freight). - ShipName – the name of the recipient (possibly the customer’s name or their receiving department). - ShipAddress , ShipCity , ShipRegion , ShipPostalCode , ShipCountry – the delivery address for the order. The presence of both billing-related info (customer and employee who took the order) and shipping info in this table shows how Northwind handles order fulfillment : an order is not only linked to who bought and sold it, but also to how it was delivered. The primary key OrderID distinguishes each order, and it is referenced by the Order Details table (meaning one order can have multiple line items). Thus, there is a one-to-many relationship from Orders to Order Details: each order can contain many products. The Orders table ties together the customer, the responsible employee, the shipment method, and timing, providing a comprehensive record for each sale .'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 9, 'section_title': 'Scenario 1: Processing a Customer Order (Part 1)', 'section_level': 3, 'char_count': 1658, 'token_count': 379, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '67e0697bbade4f3aaec02f1647023f34', '_collection_name': 'northwind_data'}, page_content='Acme Foods , a customer, places an order for 10 boxes of Chai Tea (a product). Here’s how that order is recorded and fulfilled using the database tables: An Orders record is created for the new order. This includes selecting Acme Foods’ CustomerID (linking to the Customers table to get all needed info like address), and assigning an EmployeeID for the sales representative who handles the order (linking to Employees). The OrderDate is set to 1. 9 today’s date, and a RequiredDate is set based on when Acme needs the goods. At this stage, ShippedDate is empty (not shipped yet) and a provisional ShipVia (shipper) might be chosen or left for later, with no Freight cost entered yet. Two Order Details rows are added (because the customer wants 10 boxes of Chai Tea and let’s say 5 boxes of Chang (another product) as well). Each Order Details entry uses the new Order’s OrderID, and one has ProductID for Chai Tea, quantity 10, the other has ProductID for Chang, quantity 5. The unit prices for each (at the time of order) are pulled from the Products table and recorded in Order Details. If there’s any discount (perhaps the customer has a bulk discount), that is noted in the Discount field. As soon as the order is saved, Northwind’s inventory can be updated. The system will subtract 10 units from Chai Tea ’s UnitsInStock in the Products table, and 5 units from Chang ’s UnitsInStock, to reflect that these items are allocated to this order. If any product’s stock fell below its ReorderLevel because of this order, it’s a trigger for the purchasing team to place a restock order with that product’s supplier. The order is then processed for shipping.'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 4, 'section_title': 'Orders Table (Part 2)', 'section_level': 3, 'char_count': 435, 'token_count': 101, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '807d92635838448e8e344c940c894d79', '_collection_name': 'northwind_data'}, page_content='For example, an order record might show that Order #10248 was placed by Customer ALFKI on July 4, 2025 , entered by Employee #5 , and shipped via Speedy Express on July 9, 2025 with a freight charge of $32. Freight and shipper data can be used for logistics analysis (e.g. average shipping times or costs), while the employee and customer links support sales performance tracking (e.g. orders per employee, order history per customer).'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 9, 'section_title': 'Scenario 1: Processing a Customer Order (Part 2)', 'section_level': 3, 'char_count': 1643, 'token_count': 347, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': 'f6a6ec13a4924634b62a06eb5b519294', '_collection_name': 'northwind_data'}, page_content='The staff assigns a Shipper (say Speedy Express ) – so the Orders record’s ShipVia is set to the ShipperID of Speedy Express – and packs the goods for delivery. Once shipped, they update the Orders record: set the ShippedDate to the actual ship date, and record the Freight charge (e.g. $50) based on what the shipper will bill. The shipping address fields (ShipAddress, ShipCity, etc.) in the Orders record were automatically filled with the customer’s address from Customers at order creation, but they could be edited if the customer wanted it shipped to a different location. In this case, suppose Acme Foods wanted it delivered directly to a branch office – the address fields are updated accordingly without changing the main customer address (this flexibility is why Orders stores a copy of the ship address). Finally, an invoice can be generated (in Northwind, this is often done via a query or report rather than a separate table). The invoice would pull customer info from Customers, order and line item details from Orders and Order Details, and show the totals. Once the payment is received and the process is completed, that order might be marked as closed (in the Northwind sample, there isn’t a specific status field in the table, but logically that would be the end of the cycle). In this scenario, we see Customers, Orders, Order Details, Products, Shippers, Employees (and indirectly Suppliers, due to reordering) all coming into play. The design ensures that by knowing the OrderID, one can navigate to who made the order, what they bought, who sold it, how it was shipped, and *what it cost – all through the linked tables.'),\n",
       " Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 1, 'section_title': 'Order Processing and Fulfillment', 'section_level': 3, 'char_count': 1098, 'token_count': 216, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': False, '_id': '7e4e377b79a14d26835a0d2b9bf64280', '_collection_name': 'northwind_data'}, page_content='Order processing is at the heart of Northwind’s operations. When a customer places an order, the details of that sale are recorded in the database. An entry is added to the Orders table to capture the order’s high- level information (who the customer is, which employee handled it, when it was ordered and shipped, etc.). Each individual product item in the order is recorded as a line item in the Order Details table (also called Order Items in some versions). Together, these tables model the order fulfillment process from start to finish: an order is placed by a customer, processed by an employee, shipped via a designated shipper, and eventually marked as fulfilled (with shipping dates and freight cost logged). This process is simple in the Northwind sample (e.g. inventory is assumed to be available), but it provides a solid framework for recording order status and fulfillment steps (order date, ship date, ship method, etc.). The relationships between orders, order details, customers, employees, and shippers in the schema ensure that every order can be tracked through to fulfillment.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_retriever.invoke(\"How many orders are there from the US?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "You are a helpful assistant. Use the available context to answer the question. If you can't answer the question, say you don't know.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | filtered_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | openai_chat_model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED VERSION: Enhanced retriever with section awareness and token budget management\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Any, Dict\n",
    "from pydantic import Field\n",
    "\n",
    "class FixedSectionAwareRetriever(BaseRetriever):\n",
    "    \"\"\"Enhanced retriever that considers section hierarchy and manages token budgets.\"\"\"\n",
    "    \n",
    "    vectorstore: Any = Field()\n",
    "    score_threshold: float = Field(default=0.3)\n",
    "    k: int = Field(default=10)\n",
    "    max_tokens: int = Field(default=2000)  # Token budget for context\n",
    "    prefer_high_level_sections: bool = Field(default=True)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Get initial candidates\n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=self.k * 2)\n",
    "        \n",
    "        # Filter by score threshold and keep score information\n",
    "        filtered_docs_with_scores = [\n",
    "            (doc, score) for doc, score in docs_with_scores \n",
    "            if score >= self.score_threshold\n",
    "        ]\n",
    "        \n",
    "        # If preferring high-level sections, sort by section level and score\n",
    "        if self.prefer_high_level_sections:\n",
    "            # Sort by section level (lower numbers = higher levels) and score\n",
    "            filtered_docs_with_scores.sort(key=lambda item: (item[0].metadata.get('section_level', 999), -item[1]))\n",
    "        \n",
    "        # Apply token budget management\n",
    "        selected_docs = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for doc, score in filtered_docs_with_scores:\n",
    "            doc_tokens = doc.metadata.get('token_count', len(doc.page_content) // 4)\n",
    "            \n",
    "            if total_tokens + doc_tokens <= self.max_tokens:\n",
    "                selected_docs.append(doc)\n",
    "                total_tokens += doc_tokens\n",
    "            elif len(selected_docs) == 0:  # Ensure we return at least one document\n",
    "                selected_docs.append(doc)\n",
    "                break\n",
    "        \n",
    "        return selected_docs[:self.k]\n",
    "\n",
    "# Create FIXED enhanced retriever\n",
    "enhanced_retriever = FixedSectionAwareRetriever(\n",
    "    vectorstore=qdrant_vectorstore,\n",
    "    score_threshold=0.2,  # Lower threshold for more results\n",
    "    k=8,\n",
    "    max_tokens=1500,  # Leave room for the question and response\n",
    "    prefer_high_level_sections=True\n",
    ")\n",
    "\n",
    "print(\"✅ Created FIXED section-aware retriever with token budget management\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context does not include information about employee performance or any metrics that would identify top performing employees.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"question\" : \"Who are the top performing employees?\"})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Enhanced RAG with Section-Aware Retrieval\n",
    "\n",
    "Let's create a more sophisticated retriever that can leverage our section-based chunking for better context-aware responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created section-aware retriever with token budget management\n"
     ]
    }
   ],
   "source": [
    "# Enhanced retriever with section awareness and token budget management\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Any, Dict\n",
    "from pydantic import Field\n",
    "\n",
    "class SectionAwareRetriever(BaseRetriever):\n",
    "    \"\"\"Enhanced retriever that considers section hierarchy and manages token budgets.\"\"\"\n",
    "    \n",
    "    vectorstore: Any = Field()\n",
    "    score_threshold: float = Field(default=0.3)\n",
    "    k: int = Field(default=10)\n",
    "    max_tokens: int = Field(default=2000)  # Token budget for context\n",
    "    prefer_high_level_sections: bool = Field(default=True)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Get initial candidates\n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=self.k * 2)\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_docs = [\n",
    "            doc for doc, score in docs_with_scores \n",
    "            if score >= self.score_threshold\n",
    "        ]\n",
    "        \n",
    "        # If preferring high-level sections, boost their scores\n",
    "        if self.prefer_high_level_sections:\n",
    "            # Sort by section level (lower numbers = higher levels) and score\n",
    "            filtered_docs.sort(key=lambda doc: (doc.metadata.get('section_level', 999), -docs_with_scores[filtered_docs.index(doc)][1]))\n",
    "        \n",
    "        # Apply token budget management\n",
    "        selected_docs = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for doc in filtered_docs:\n",
    "            doc_tokens = doc.metadata.get('token_count', len(doc.page_content) // 4)\n",
    "            \n",
    "            if total_tokens + doc_tokens <= self.max_tokens:\n",
    "                selected_docs.append(doc)\n",
    "                total_tokens += doc_tokens\n",
    "            elif len(selected_docs) == 0:  # Ensure we return at least one document\n",
    "                selected_docs.append(doc)\n",
    "                break\n",
    "        \n",
    "        return selected_docs[:self.k]\n",
    "\n",
    "# Create enhanced retriever\n",
    "enhanced_retriever = SectionAwareRetriever(\n",
    "    vectorstore=qdrant_vectorstore,\n",
    "    score_threshold=0.2,  # Lower threshold for more results\n",
    "    k=8,\n",
    "    max_tokens=1500,  # Leave room for the question and response\n",
    "    prefer_high_level_sections=True\n",
    ")\n",
    "\n",
    "print(\"✅ Created section-aware retriever with token budget management\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created enhanced RAG chain with section-aware formatting\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG prompt that leverages section information\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser  # Add missing import\n",
    "\n",
    "ENHANCED_RAG_PROMPT = \"\"\"\n",
    "CONTEXT SECTIONS:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "You are an expert assistant with access to the Northwind Traders Database Overview documentation. \n",
    "The context above is organized by document sections to preserve the logical structure of the information.\n",
    "\n",
    "When answering:\n",
    "1. Reference specific sections when relevant (e.g., \"According to the [Section Name] section...\")\n",
    "2. Maintain the hierarchical structure of information in your response\n",
    "3. If information spans multiple sections, clearly indicate this\n",
    "4. Use the section titles to understand the context and scope of each piece of information\n",
    "5. If you cannot answer the question based on the provided sections, say so clearly\n",
    "\n",
    "Answer the question using the available context sections:\n",
    "\"\"\"\n",
    "\n",
    "def format_context_with_sections(docs):\n",
    "    \"\"\"Format retrieved documents to show section hierarchy.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant sections found.\"\n",
    "    \n",
    "    formatted_sections = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        section_title = doc.metadata.get('section_title', f'Section {i+1}')\n",
    "        section_level = doc.metadata.get('section_level', 0)\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        token_count = doc.metadata.get('token_count', 0)\n",
    "        \n",
    "        # Create hierarchical indicator\n",
    "        level_indicator = \"  \" * (section_level - 1) + f\"Level {section_level}: \" if section_level > 0 else \"\"\n",
    "        \n",
    "        section_header = f\"[{level_indicator}{section_title} (Page {page}, {token_count} tokens)]\"\n",
    "        formatted_sections.append(f\"{section_header}\\n{doc.page_content.strip()}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(formatted_sections)\n",
    "\n",
    "# Create enhanced RAG chain\n",
    "enhanced_rag_prompt = ChatPromptTemplate.from_template(ENHANCED_RAG_PROMPT)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "enhanced_rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | enhanced_retriever | RunnableLambda(format_context_with_sections),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | enhanced_rag_prompt \n",
    "    | openai_chat_model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ Created enhanced RAG chain with section-aware formatting\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Test the Enhanced RAG System\n",
    "\n",
    "Let's test our new section-aware RAG system and compare it with the basic approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Testing Enhanced Section-Aware RAG:\n",
      "Question: What are the main entities and relationships in the Northwind database?\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 3, 'section_title': 'Schema Walkthrough: Key Tables and Relationships', 'section_level': 1, 'char_count': 1192, 'token_count': 228, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': False, '_id': '4712e9c8418f495eb4709d5c40692b84', '_collection_name': 'northwind_data'}, page_content='The Northwind database is a relational schema with multiple tables, each representing an entity in the business. The design is highly normalized – information is broken into logical tables with relationships (via primary and foreign keys) connecting them. Below, we walk through each of the key tables, explaining their purpose, structure, and how they interrelate. Figure: Entity-Relationship Diagram (ERD) of the Northwind database schema, showing the key tables and their relationships. Each box is a table (with major columns listed), and lines indicate foreign key relationships between tables. For example, the Orders table links to the Customers table (each order is placed by one customer) and to the Employees table (each order is handled by one employee). Orders connect to Order Details in a one-to-many relationship (one order has many detail line items), and Order Details in turn link each product sold (tying into the Products table). The diagram also shows how each Product belongs to a Category and is supplied by a Supplier. Employees are linked to Territories (and their Regions) through the EmployeeTerritories table, illustrating the assignment of sales regions to staff.') is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get the response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m enhanced_response = \u001b[43menhanced_rag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(enhanced_response)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/retrievers.py:263\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\n\u001b[32m    260\u001b[39m             \u001b[38;5;28minput\u001b[39m, run_manager=run_manager, **_kwargs\n\u001b[32m    261\u001b[39m         )\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    265\u001b[39m     run_manager.on_retriever_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mSectionAwareRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# If preferring high-level sections, boost their scores\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prefer_high_level_sections:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Sort by section level (lower numbers = higher levels) and score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_level\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mdocs_with_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Apply token budget management\u001b[39;00m\n\u001b[32m     34\u001b[39m selected_docs = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mSectionAwareRetriever._get_relevant_documents.<locals>.<lambda>\u001b[39m\u001b[34m(doc)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# If preferring high-level sections, boost their scores\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prefer_high_level_sections:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Sort by section level (lower numbers = higher levels) and score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     filtered_docs.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m doc: (doc.metadata.get(\u001b[33m'\u001b[39m\u001b[33msection_level\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m999\u001b[39m), -docs_with_scores[\u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m][\u001b[32m1\u001b[39m]))\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Apply token budget management\u001b[39;00m\n\u001b[32m     34\u001b[39m selected_docs = []\n",
      "\u001b[31mValueError\u001b[39m: Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 3, 'section_title': 'Schema Walkthrough: Key Tables and Relationships', 'section_level': 1, 'char_count': 1192, 'token_count': 228, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': False, '_id': '4712e9c8418f495eb4709d5c40692b84', '_collection_name': 'northwind_data'}, page_content='The Northwind database is a relational schema with multiple tables, each representing an entity in the business. The design is highly normalized – information is broken into logical tables with relationships (via primary and foreign keys) connecting them. Below, we walk through each of the key tables, explaining their purpose, structure, and how they interrelate. Figure: Entity-Relationship Diagram (ERD) of the Northwind database schema, showing the key tables and their relationships. Each box is a table (with major columns listed), and lines indicate foreign key relationships between tables. For example, the Orders table links to the Customers table (each order is placed by one customer) and to the Employees table (each order is handled by one employee). Orders connect to Order Details in a one-to-many relationship (one order has many detail line items), and Order Details in turn link each product sold (tying into the Products table). The diagram also shows how each Product belongs to a Category and is supplied by a Supplier. Employees are linked to Territories (and their Regions) through the EmployeeTerritories table, illustrating the assignment of sales regions to staff.') is not in list"
     ]
    }
   ],
   "source": [
    "# Test the enhanced RAG system\n",
    "test_question = \"What are the main entities and relationships in the Northwind database?\"\n",
    "\n",
    "print(\"🤖 Testing Enhanced Section-Aware RAG:\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Get the response\n",
    "enhanced_response = enhanced_rag_chain.invoke({\"question\": test_question})\n",
    "print(enhanced_response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Let's also show what sections were retrieved\n",
    "retrieved_docs = enhanced_retriever.invoke(test_question)\n",
    "print(f\"\\n📚 Retrieved {len(retrieved_docs)} sections:\")\n",
    "\n",
    "total_context_tokens = 0\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    section_title = doc.metadata.get('section_title', f'Section {i+1}')\n",
    "    page = doc.metadata.get('page', '?')\n",
    "    level = doc.metadata.get('section_level', 0)\n",
    "    tokens = doc.metadata.get('token_count', 0)\n",
    "    total_context_tokens += tokens\n",
    "    \n",
    "    print(f\"  {i+1}. Level {level}: '{section_title}' (Page {page}, {tokens} tokens)\")\n",
    "\n",
    "print(f\"\\n🔢 Total context tokens used: {total_context_tokens}\")\n",
    "print(f\"💡 Token efficiency: Using only {total_context_tokens} tokens for comprehensive context!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Enhanced RAG with Various Questions:\n",
      "====================================================================================================\n",
      "\n",
      "🔍 Question 1: Who are the top performing employees in Northwind?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 7, 'section_title': 'Employees Table (Part 2)', 'section_level': 3, 'char_count': 643, 'token_count': 120, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '65d810e4a688475ea5bec93baf2f9ebe', '_collection_name': 'northwind_data'}, page_content='By querying this, one can tally each employee’s sales or see which customers each employee has dealt with. The Employees table combined with territory assignments (described next) also allows Northwind to map its workforce to sales regions. Additionally, from an analytics or HR perspective, having all these details enables the company to track employee performance (sales figures by employee via Orders linkage), tenure (via HireDate), and other metrics. In summary, the Employees table defines who works at Northwind, what their role is, and connects them both to the organizational hierarchy and to the sales activities in the Orders data.') is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43menhanced_rag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Show retrieved sections\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/langchain_core/retrievers.py:263\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\n\u001b[32m    260\u001b[39m             \u001b[38;5;28minput\u001b[39m, run_manager=run_manager, **_kwargs\n\u001b[32m    261\u001b[39m         )\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    265\u001b[39m     run_manager.on_retriever_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mSectionAwareRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# If preferring high-level sections, boost their scores\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prefer_high_level_sections:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Sort by section level (lower numbers = higher levels) and score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_level\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mdocs_with_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Apply token budget management\u001b[39;00m\n\u001b[32m     34\u001b[39m selected_docs = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mSectionAwareRetriever._get_relevant_documents.<locals>.<lambda>\u001b[39m\u001b[34m(doc)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# If preferring high-level sections, boost their scores\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prefer_high_level_sections:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Sort by section level (lower numbers = higher levels) and score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     filtered_docs.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m doc: (doc.metadata.get(\u001b[33m'\u001b[39m\u001b[33msection_level\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m999\u001b[39m), -docs_with_scores[\u001b[43mfiltered_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m][\u001b[32m1\u001b[39m]))\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Apply token budget management\u001b[39;00m\n\u001b[32m     34\u001b[39m selected_docs = []\n",
      "\u001b[31mValueError\u001b[39m: Document(metadata={'source': 'data/Northwind_Traders_Database_Overview.pdf', 'page': 7, 'section_title': 'Employees Table (Part 2)', 'section_level': 3, 'char_count': 643, 'token_count': 120, 'chunking_method': 'tokens', 'encoding': 'cl100k_base', 'chunk_type': 'section', 'is_split': True, '_id': '65d810e4a688475ea5bec93baf2f9ebe', '_collection_name': 'northwind_data'}, page_content='By querying this, one can tally each employee’s sales or see which customers each employee has dealt with. The Employees table combined with territory assignments (described next) also allows Northwind to map its workforce to sales regions. Additionally, from an analytics or HR perspective, having all these details enables the company to track employee performance (sales figures by employee via Orders linkage), tenure (via HireDate), and other metrics. In summary, the Employees table defines who works at Northwind, what their role is, and connects them both to the organizational hierarchy and to the sales activities in the Orders data.') is not in list"
     ]
    }
   ],
   "source": [
    "# Test with different types of questions to showcase the system's capabilities\n",
    "\n",
    "test_questions = [\n",
    "    \"Who are the top performing employees in Northwind?\",\n",
    "    \"What products does Northwind sell and how are they categorized?\",\n",
    "    \"Describe the customer base and their geographic distribution\",\n",
    "    \"How does the order processing system work?\",\n",
    "    \"What are the key business relationships in the Northwind database model?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Enhanced RAG with Various Questions:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n🔍 Question {i}: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get response\n",
    "    response = enhanced_rag_chain.invoke({\"question\": question})\n",
    "    print(response)\n",
    "    \n",
    "    # Show retrieved sections\n",
    "    docs = enhanced_retriever.invoke(question)\n",
    "    section_info = [f\"{doc.metadata.get('section_title', 'Unknown')} (Level {doc.metadata.get('section_level', 0)})\" \n",
    "                   for doc in docs]\n",
    "    tokens_used = sum(doc.metadata.get('token_count', 0) for doc in docs)\n",
    "    \n",
    "    print(f\"\\n📊 Retrieved sections: {', '.join(section_info[:3])}{'...' if len(section_info) > 3 else ''}\")\n",
    "    print(f\"🔢 Context tokens: {tokens_used}\")\n",
    "    \n",
    "    if i < len(test_questions):\n",
    "        print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Summary: Enhanced PDF Chunking Benefits\n",
    "\n",
    "Our section-based PDF chunking with tiktoken support provides several key advantages over basic text splitting:\n",
    "\n",
    "## 🎯 **Intelligent Structure Preservation**\n",
    "- Maintains the logical hierarchy of the PDF document\n",
    "- Preserves section boundaries and relationships\n",
    "- Enables section-aware retrieval\n",
    "\n",
    "## 🔢 **Token-Optimized Chunking**\n",
    "- Precise token counting using tiktoken\n",
    "- Optimized for OpenAI embedding models\n",
    "- Better control over context windows\n",
    "\n",
    "## 📊 **Enhanced Metadata**\n",
    "- Section titles and levels\n",
    "- Page numbers and source tracking\n",
    "- Token counts and character counts\n",
    "- Chunk quality indicators\n",
    "\n",
    "## ⚡ **Improved RAG Performance**\n",
    "- More relevant context retrieval\n",
    "- Better token budget management\n",
    "- Hierarchical result ranking\n",
    "- Reduced hallucination through structured context\n",
    "\n",
    "## 🚀 **Production Ready**\n",
    "- Works with any structured PDF document\n",
    "- Configurable parameters for different use cases\n",
    "- Comprehensive error handling\n",
    "- Integration-friendly format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser  # Add missing import\n",
    "\n",
    "RAG_PROMPT_SUMMARY = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ORIGINAL QUERY:\n",
    "{question}\n",
    "\n",
    "You are a document summarizer. Analyze the provided context and create a summary that addresses the query as best as possible.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- If the context contains information about customers, orders, or sales data, extract and summarize it\n",
    "- Even if the exact query cannot be answered, summarize any related information found\n",
    "- If you find customer data but not specifically \"top customers\", still summarize what customer information is available\n",
    "- Keep the summary under 200 words\n",
    "- Only return \"No relevant information found\" if the context is completely unrelated to the query topic\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_summary = ChatPromptTemplate.from_template(RAG_PROMPT_SUMMARY)\n",
    "\n",
    "# rag_chain_summary = (\n",
    "#     {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
    "#     | rag_prompt_summary | openai_chat_model | StrOutputParser()\n",
    "# )\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def format_and_filter_docs(docs):\n",
    "    \"\"\"Format docs and add metadata for better summarization\"\"\"\n",
    "    if not docs:\n",
    "        return \"No documents retrieved.\"\n",
    "    \n",
    "    formatted_context = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # Add source info if available\n",
    "        source = doc.metadata.get('source', f'Document {i+1}')\n",
    "        content = doc.page_content.strip()\n",
    "        formatted_context.append(f\"[{source}]: {content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_context)\n",
    "\n",
    "# Your summary chain\n",
    "rag_chain_summary = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | qdrant_retriever | RunnableLambda(format_and_filter_docs),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_prompt_summary \n",
    "    | openai_chat_model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context describes the Northwind sample database, which contains 14 interrelated tables relevant to a fictitious trading business, including data on orders, customers, and products. Specifically, the Orders table captures sales orders placed by customers, indicating a potential source for identifying customer activity and sales. However, the context does not include specific data or metrics regarding the top customers in the US or any sales figures that could be leveraged to ascertain customer rankings. Additionally, it references the CustomerDemographics table without providing details about its content. Overall, while the Northwind database features comprehensive customer and order information that could lead to identifying top customers, the exact details of those customers are not presented in the context provided.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_summary.invoke({\"question\" : \"Who are the top customers of Northwind in the US?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores:\n",
      "Score: 0.579\n",
      "Content: 42\n",
      "\n",
      "43\n",
      "\n",
      "44\n",
      "\n",
      "45\n",
      "\n",
      "46\n",
      "\n",
      "47\n",
      "\n",
      "48\n",
      "\n",
      "49\n",
      "\n",
      "Northwind Database\n",
      "\n",
      "https://techwriter.me/downloads/samples/Database/Access2003Northwind.pdf\n",
      "\n",
      "30\n",
      "\n",
      "51\n",
      "\n",
      "52\n",
      "\n",
      "53\n",
      "\n",
      "54\n",
      "\n",
      "55\n",
      "\n",
      "58\n",
      "\n",
      "59\n",
      "\n",
      "60\n",
      "\n",
      "raw.githubusercontent.com\n",
      "\n",
      "https://raw...\n",
      "---\n",
      "Score: 0.548\n",
      "Content: Northwind Database Schema\n",
      "\n",
      "Overview: The Northwind sample database contains 14 interrelated tables representing a fictitious trading\n",
      "\n",
      "business. The schema includes tables for orders, customers, employ...\n",
      "---\n",
      "Score: 0.485\n",
      "Content: 17\n",
      "\n",
      ". Similarly, each Product can appear in many\n",
      "\n",
      "Order_Details records (one-to-many from Products to Order_Details)\n",
      "\n",
      "18\n",
      "\n",
      ". Through Order_Details,\n",
      "\n",
      "Orders and Products are related in a many-to-many ma...\n",
      "---\n",
      "Score: 0.470\n",
      "Content: Foreign Keys: (None) – US_States is a standalone reference table.\n",
      "\n",
      "\n",
      "\n",
      "Relationships: This table is not linked via foreign keys to the rest of the Northwind schema. It may\n",
      "\n",
      "be used for address data cons...\n",
      "---\n",
      "Score: 0.464\n",
      "Content: Foreign Keys: (None) – the Customers table does not have foreign key columns; it stands alone and\n",
      "\n",
      "is referenced by other tables.\n",
      "\n",
      "\n",
      "\n",
      "Relationships: One Customer can place many Orders (Customers:Orders...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "docs_with_scores = qdrant_vectorstore.similarity_search_with_score(\n",
    "    \"Who are the top customers of Northwind in the US?\", k=5\n",
    ")\n",
    "\n",
    "print(\"Similarity scores:\")\n",
    "for doc, score in docs_with_scores:\n",
    "    print(f\"Score: {score:.3f}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "def create_comprehensive_northwind_business_documents(\n",
    "    host: str,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    database: str = \"neondb\",\n",
    "    port: int = 5432,\n",
    "    schema: str = \"northwind\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Execute comprehensive SQL queries against PostgreSQL Northwind database hosted on Neon \n",
    "    and create detailed business-friendly documents for vector search and RAG applications.\n",
    "    \n",
    "    This expanded version includes deep analysis of all business aspects including:\n",
    "    - Customer demographics and behavior patterns\n",
    "    - Product performance and inventory management\n",
    "    - Employee productivity and territory analysis\n",
    "    - Supplier relationships and logistics\n",
    "    - Financial performance and trends\n",
    "    - Geographic distribution and shipping patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create connection string for Neon\n",
    "    conn_string = f\"postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=require\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Connecting to Northwind database and generating comprehensive business documents...\")\n",
    "        \n",
    "        # 1. COMPREHENSIVE CUSTOMER ANALYSIS\n",
    "        print(\"Generating customer analysis document...\")\n",
    "        \n",
    "        # Customer demographics and distribution\n",
    "        customer_demographics_query = f\"\"\"\n",
    "        SELECT \n",
    "            country,\n",
    "            city,\n",
    "            COUNT(*) as customer_count,\n",
    "            STRING_AGG(DISTINCT contact_title, ', ') as job_titles,\n",
    "            STRING_AGG(company_name, '; ' ORDER BY company_name) as sample_companies\n",
    "        FROM {schema}.customers \n",
    "        GROUP BY country, city\n",
    "        ORDER BY customer_count DESC, country, city\n",
    "        \"\"\"\n",
    "        \n",
    "        df_demographics = pd.read_sql_query(customer_demographics_query, conn_string)\n",
    "        \n",
    "        customer_doc = \"NORTHWIND COMPREHENSIVE CUSTOMER ANALYSIS:\\n\\n\"\n",
    "        customer_doc += f\"CUSTOMER BASE OVERVIEW:\\n\"\n",
    "        customer_doc += f\"Northwind serves {df_demographics['customer_count'].sum()} customers across {len(df_demographics['country'].unique())} countries and {len(df_demographics)} cities.\\n\\n\"\n",
    "        \n",
    "        # Country analysis\n",
    "        country_summary = df_demographics.groupby('country').agg({\n",
    "            'customer_count': 'sum',\n",
    "            'city': 'count'\n",
    "        }).sort_values('customer_count', ascending=False)\n",
    "        \n",
    "        customer_doc += \"CUSTOMER DISTRIBUTION BY COUNTRY:\\n\"\n",
    "        for country, row in country_summary.head(15).iterrows():\n",
    "            customer_doc += f\"- {country}: {row['customer_count']} customers across {row['city']} cities\\n\"\n",
    "        \n",
    "        # Detailed customer profiles with contact information\n",
    "        detailed_customers_query = f\"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            company_name,\n",
    "            contact_name,\n",
    "            contact_title,\n",
    "            city,\n",
    "            region,\n",
    "            country,\n",
    "            phone,\n",
    "            fax\n",
    "        FROM {schema}.customers\n",
    "        ORDER BY country, city, company_name\n",
    "        \"\"\"\n",
    "        \n",
    "        df_detailed = pd.read_sql_query(detailed_customers_query, conn_string)\n",
    "        \n",
    "        customer_doc += \"\\n\\nDETAILED CUSTOMER DIRECTORY:\\n\"\n",
    "        \n",
    "        # Group by country for better organization\n",
    "        for country in df_detailed['country'].unique()[:10]:  # Top 10 countries\n",
    "            country_customers = df_detailed[df_detailed['country'] == country]\n",
    "            customer_doc += f\"\\n{country.upper()} ({len(country_customers)} customers):\\n\"\n",
    "            \n",
    "            for _, customer in country_customers.head(8).iterrows():  # Top 8 per country\n",
    "                region_info = f\", {customer['region']}\" if pd.notna(customer['region']) else \"\"\n",
    "                fax_info = f\", Fax: {customer['fax']}\" if pd.notna(customer['fax']) else \"\"\n",
    "                customer_doc += f\"  • {customer['company_name']} - {customer['contact_name']} ({customer['contact_title']})\\n\"\n",
    "                customer_doc += f\"    Location: {customer['city']}{region_info}, Phone: {customer['phone']}{fax_info}\\n\"\n",
    "        \n",
    "        documents.append(customer_doc)\n",
    "        \n",
    "        # 2. CUSTOMER PURCHASING BEHAVIOR AND TOP PERFORMERS\n",
    "        print(\"Generating customer purchasing behavior analysis...\")\n",
    "        \n",
    "        customer_behavior_query = f\"\"\"\n",
    "        WITH customer_metrics AS (\n",
    "            SELECT \n",
    "                c.customer_id,\n",
    "                c.company_name,\n",
    "                c.contact_name,\n",
    "                c.city,\n",
    "                c.region,\n",
    "                c.country,\n",
    "                COUNT(DISTINCT o.order_id) as total_orders,\n",
    "                COUNT(DISTINCT DATE_PART('year', o.order_date)) as years_active,\n",
    "                MIN(o.order_date) as first_order_date,\n",
    "                MAX(o.order_date) as last_order_date,\n",
    "                ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as total_revenue,\n",
    "                ROUND(AVG(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as avg_order_value,\n",
    "                SUM(od.quantity) as total_items_purchased,\n",
    "                ROUND(AVG(o.freight)::numeric, 2) as avg_shipping_cost,\n",
    "                COUNT(DISTINCT od.product_id) as unique_products_bought\n",
    "            FROM {schema}.customers c\n",
    "            JOIN {schema}.orders o ON c.customer_id = o.customer_id\n",
    "            JOIN {schema}.order_details od ON o.order_id = od.order_id\n",
    "            GROUP BY c.customer_id, c.company_name, c.contact_name, c.city, c.region, c.country\n",
    "        )\n",
    "        SELECT * FROM customer_metrics\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_behavior = pd.read_sql_query(customer_behavior_query, conn_string)\n",
    "        \n",
    "        behavior_doc = \"NORTHWIND CUSTOMER PURCHASING BEHAVIOR ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Top customers by revenue\n",
    "        behavior_doc += \"TOP 20 CUSTOMERS BY TOTAL REVENUE:\\n\"\n",
    "        for i, row in df_behavior.head(20).iterrows():\n",
    "            region = f\", {row['region']}\" if pd.notna(row['region']) else \"\"\n",
    "            years_span = f\"{row['first_order_date']:.10}\" + \" to \" + f\"{row['last_order_date']:.10}\"\n",
    "            \n",
    "            behavior_doc += f\"{i+1}. {row['company_name']} ({row['country']})\\n\"\n",
    "            behavior_doc += f\"   • Total Revenue: ${row['total_revenue']:,.2f} across {row['total_orders']} orders\\n\"\n",
    "            behavior_doc += f\"   • Average Order Value: ${row['avg_order_value']:,.2f}\\n\"\n",
    "            behavior_doc += f\"   • Active Period: {years_span} ({row['years_active']} years)\\n\"\n",
    "            behavior_doc += f\"   • Location: {row['city']}{region}\\n\"\n",
    "            behavior_doc += f\"   • Products Diversity: {row['unique_products_bought']} different products, {row['total_items_purchased']} total items\\n\\n\"\n",
    "        \n",
    "        # Customer segmentation analysis\n",
    "        behavior_doc += \"CUSTOMER SEGMENTATION ANALYSIS:\\n\"\n",
    "        revenue_percentiles = df_behavior['total_revenue'].quantile([0.25, 0.5, 0.75, 0.9, 0.95])\n",
    "        \n",
    "        behavior_doc += f\"• Premium Customers (Top 5%): ${revenue_percentiles[0.95]:,.2f}+ revenue ({len(df_behavior[df_behavior['total_revenue'] >= revenue_percentiles[0.95]])} customers)\\n\"\n",
    "        behavior_doc += f\"• High-Value Customers (Top 10%): ${revenue_percentiles[0.9]:,.2f}+ revenue ({len(df_behavior[df_behavior['total_revenue'] >= revenue_percentiles[0.9]])} customers)\\n\"\n",
    "        behavior_doc += f\"• Regular Customers (Median): ${revenue_percentiles[0.5]:,.2f} revenue\\n\"\n",
    "        behavior_doc += f\"• Average Order Value Range: ${df_behavior['avg_order_value'].min():,.2f} - ${df_behavior['avg_order_value'].max():,.2f}\\n\"\n",
    "        \n",
    "        # Geographic revenue distribution\n",
    "        geographic_revenue = df_behavior.groupby('country').agg({\n",
    "            'total_revenue': 'sum',\n",
    "            'total_orders': 'sum',\n",
    "            'company_name': 'count'\n",
    "        }).sort_values('total_revenue', ascending=False)\n",
    "        \n",
    "        behavior_doc += \"\\nREVENUE BY COUNTRY:\\n\"\n",
    "        for country, row in geographic_revenue.head(10).iterrows():\n",
    "            avg_revenue_per_customer = row['total_revenue'] / row['company_name']\n",
    "            behavior_doc += f\"• {country}: ${row['total_revenue']:,.2f} total (${avg_revenue_per_customer:,.2f} avg per customer)\\n\"\n",
    "        \n",
    "        documents.append(behavior_doc)\n",
    "        \n",
    "        # 3. COMPREHENSIVE PRODUCT CATALOG AND PERFORMANCE\n",
    "        print(\"Generating comprehensive product analysis...\")\n",
    "        \n",
    "        product_analysis_query = f\"\"\"\n",
    "        WITH product_performance AS (\n",
    "            SELECT \n",
    "                p.product_id,\n",
    "                p.product_name,\n",
    "                c.category_name,\n",
    "                s.company_name as supplier_name,\n",
    "                s.country as supplier_country,\n",
    "                p.quantity_per_unit,\n",
    "                p.unit_price,\n",
    "                p.units_in_stock,\n",
    "                p.units_on_order,\n",
    "                p.reorder_level,\n",
    "                p.discontinued,\n",
    "                COALESCE(SUM(od.quantity), 0) as total_quantity_sold,\n",
    "                COALESCE(ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2), 0) as total_revenue,\n",
    "                COALESCE(COUNT(DISTINCT od.order_id), 0) as orders_count,\n",
    "                COALESCE(ROUND(AVG(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2), 0) as avg_order_line_value\n",
    "            FROM {schema}.products p\n",
    "            JOIN {schema}.categories c ON p.category_id = c.category_id\n",
    "            JOIN {schema}.suppliers s ON p.supplier_id = s.supplier_id\n",
    "            LEFT JOIN {schema}.order_details od ON p.product_id = od.product_id\n",
    "            GROUP BY p.product_id, p.product_name, c.category_name, s.company_name, s.country,\n",
    "                     p.quantity_per_unit, p.unit_price, p.units_in_stock, p.units_on_order, \n",
    "                     p.reorder_level, p.discontinued\n",
    "        )\n",
    "        SELECT * FROM product_performance\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(product_analysis_query, conn_string)\n",
    "        \n",
    "        product_doc = \"NORTHWIND COMPREHENSIVE PRODUCT CATALOG AND PERFORMANCE:\\n\\n\"\n",
    "        \n",
    "        # Category overview\n",
    "        category_performance = df_products.groupby('category_name').agg({\n",
    "            'product_id': 'count',\n",
    "            'total_revenue': 'sum',\n",
    "            'total_quantity_sold': 'sum',\n",
    "            'unit_price': 'mean',\n",
    "            'units_in_stock': 'sum'\n",
    "        }).sort_values('total_revenue', ascending=False)\n",
    "        \n",
    "        product_doc += f\"PRODUCT PORTFOLIO OVERVIEW:\\n\"\n",
    "        product_doc += f\"Total Products: {len(df_products)} across {len(category_performance)} categories\\n\"\n",
    "        product_doc += f\"Active Products: {len(df_products[df_products['discontinued'] == 0])}\\n\"\n",
    "        product_doc += f\"Discontinued Products: {len(df_products[df_products['discontinued'] == 1])}\\n\\n\"\n",
    "        \n",
    "        product_doc += \"CATEGORY PERFORMANCE ANALYSIS:\\n\"\n",
    "        for category, row in category_performance.iterrows():\n",
    "            product_doc += f\"• {category}: {row['product_id']} products, ${row['total_revenue']:,.2f} revenue\\n\"\n",
    "            product_doc += f\"  - {row['total_quantity_sold']:,.0f} units sold, avg price: ${row['unit_price']:,.2f}\\n\"\n",
    "            product_doc += f\"  - Current inventory: {row['units_in_stock']:,.0f} units\\n\"\n",
    "        \n",
    "        # Top performing products\n",
    "        product_doc += \"\\nTOP 25 PRODUCTS BY REVENUE:\\n\"\n",
    "        for i, row in df_products.head(25).iterrows():\n",
    "            discontinued_status = \" [DISCONTINUED]\" if row['discontinued'] == 1 else \"\"\n",
    "            stock_status = \"⚠️ LOW STOCK\" if row['units_in_stock'] <= row['reorder_level'] else \"✅ IN STOCK\"\n",
    "            \n",
    "            product_doc += f\"{i+1}. {row['product_name']} ({row['category_name']}){discontinued_status}\\n\"\n",
    "            product_doc += f\"   • Revenue: ${row['total_revenue']:,.2f} from {row['total_quantity_sold']} units sold\\n\"\n",
    "            product_doc += f\"   • Price: ${row['unit_price']:.2f} per {row['quantity_per_unit']}\\n\"\n",
    "            product_doc += f\"   • Supplier: {row['supplier_name']} ({row['supplier_country']})\\n\"\n",
    "            product_doc += f\"   • Inventory: {row['units_in_stock']} in stock, {row['units_on_order']} on order ({stock_status})\\n\\n\"\n",
    "        \n",
    "        # Inventory management insights\n",
    "        low_stock_products = df_products[df_products['units_in_stock'] <= df_products['reorder_level']]\n",
    "        high_revenue_low_stock = low_stock_products[low_stock_products['total_revenue'] > df_products['total_revenue'].median()]\n",
    "        \n",
    "        product_doc += f\"INVENTORY MANAGEMENT ALERTS:\\n\"\n",
    "        product_doc += f\"• Products below reorder level: {len(low_stock_products)}\\n\"\n",
    "        product_doc += f\"• High-revenue products with low stock: {len(high_revenue_low_stock)}\\n\"\n",
    "        \n",
    "        if len(high_revenue_low_stock) > 0:\n",
    "            product_doc += \"  Critical reorder needed for:\\n\"\n",
    "            for _, product in high_revenue_low_stock.head(5).iterrows():\n",
    "                product_doc += f\"    - {product['product_name']}: {product['units_in_stock']} units (${product['total_revenue']:,.0f} revenue)\\n\"\n",
    "        \n",
    "        documents.append(product_doc)\n",
    "        \n",
    "        # 4. SUPPLIER RELATIONSHIPS AND LOGISTICS\n",
    "        print(\"Generating supplier analysis...\")\n",
    "        \n",
    "        supplier_analysis_query = f\"\"\"\n",
    "        WITH supplier_metrics AS (\n",
    "            SELECT \n",
    "                s.supplier_id,\n",
    "                s.company_name,\n",
    "                s.contact_name,\n",
    "                s.contact_title,\n",
    "                s.city,\n",
    "                s.region,\n",
    "                s.country,\n",
    "                s.phone,\n",
    "                s.fax,\n",
    "                COUNT(p.product_id) as products_supplied,\n",
    "                COUNT(CASE WHEN p.discontinued = 0 THEN 1 END) as active_products,\n",
    "                ROUND(AVG(p.unit_price)::numeric, 2) as avg_product_price,\n",
    "                SUM(p.units_in_stock) as total_inventory_units,\n",
    "                COALESCE(SUM(od.quantity), 0) as total_units_sold,\n",
    "                COALESCE(ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2), 0) as total_revenue_generated\n",
    "            FROM {schema}.suppliers s\n",
    "            JOIN {schema}.products p ON s.supplier_id = p.supplier_id\n",
    "            LEFT JOIN {schema}.order_details od ON p.product_id = od.product_id\n",
    "            GROUP BY s.supplier_id, s.company_name, s.contact_name, s.contact_title,\n",
    "                     s.city, s.region, s.country, s.phone, s.fax\n",
    "        )\n",
    "        SELECT * FROM supplier_metrics\n",
    "        ORDER BY total_revenue_generated DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_suppliers = pd.read_sql_query(supplier_analysis_query, conn_string)\n",
    "        \n",
    "        supplier_doc = \"NORTHWIND SUPPLIER RELATIONSHIP AND LOGISTICS ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Supplier overview\n",
    "        supplier_doc += f\"SUPPLIER NETWORK OVERVIEW:\\n\"\n",
    "        supplier_doc += f\"Total Suppliers: {len(df_suppliers)}\\n\"\n",
    "        supplier_doc += f\"Geographic Distribution: {len(df_suppliers['country'].unique())} countries\\n\"\n",
    "        supplier_doc += f\"Total Products Supplied: {df_suppliers['products_supplied'].sum()}\\n\"\n",
    "        supplier_doc += f\"Active Products: {df_suppliers['active_products'].sum()}\\n\\n\"\n",
    "        \n",
    "        # Supplier performance ranking\n",
    "        supplier_doc += \"TOP SUPPLIERS BY REVENUE GENERATION:\\n\"\n",
    "        for i, row in df_suppliers.head(15).iterrows():\n",
    "            region_info = f\", {row['region']}\" if pd.notna(row['region']) else \"\"\n",
    "            fax_info = f\", Fax: {row['fax']}\" if pd.notna(row['fax']) else \"\"\n",
    "            \n",
    "            supplier_doc += f\"{i+1}. {row['company_name']} ({row['country']})\\n\"\n",
    "            supplier_doc += f\"   • Contact: {row['contact_name']} ({row['contact_title']})\\n\"\n",
    "            supplier_doc += f\"   • Location: {row['city']}{region_info}\\n\"\n",
    "            supplier_doc += f\"   • Phone: {row['phone']}{fax_info}\\n\"\n",
    "            supplier_doc += f\"   • Products: {row['products_supplied']} total ({row['active_products']} active)\\n\"\n",
    "            supplier_doc += f\"   • Revenue Generated: ${row['total_revenue_generated']:,.2f}\\n\"\n",
    "            supplier_doc += f\"   • Units Sold: {row['total_units_sold']:,.0f}, Avg Product Price: ${row['avg_product_price']:,.2f}\\n\\n\"\n",
    "        \n",
    "        # Geographic supplier distribution\n",
    "        supplier_by_country = df_suppliers.groupby('country').agg({\n",
    "            'supplier_id': 'count',\n",
    "            'products_supplied': 'sum',\n",
    "            'total_revenue_generated': 'sum'\n",
    "        }).sort_values('total_revenue_generated', ascending=False)\n",
    "        \n",
    "        supplier_doc += \"SUPPLIER GEOGRAPHIC DISTRIBUTION:\\n\"\n",
    "        for country, row in supplier_by_country.iterrows():\n",
    "            avg_revenue_per_supplier = row['total_revenue_generated'] / row['supplier_id']\n",
    "            supplier_doc += f\"• {country}: {row['supplier_id']} suppliers, {row['products_supplied']} products, ${row['total_revenue_generated']:,.2f} revenue\\n\"\n",
    "            supplier_doc += f\"  Average revenue per supplier: ${avg_revenue_per_supplier:,.2f}\\n\"\n",
    "        \n",
    "        documents.append(supplier_doc)\n",
    "        \n",
    "        # 5. EMPLOYEE PERFORMANCE AND TERRITORY ANALYSIS\n",
    "        print(\"Generating employee and territory analysis...\")\n",
    "        \n",
    "        employee_analysis_query = f\"\"\"\n",
    "        WITH employee_performance AS (\n",
    "            SELECT \n",
    "                e.employee_id,\n",
    "                e.first_name || ' ' || e.last_name as full_name,\n",
    "                e.title,\n",
    "                e.title_of_courtesy,\n",
    "                e.birth_date,\n",
    "                e.hire_date,\n",
    "                e.city,\n",
    "                e.region,\n",
    "                e.country,\n",
    "                e.home_phone,\n",
    "                e.reports_to,\n",
    "                mgr.first_name || ' ' || mgr.last_name as manager_name,\n",
    "                COUNT(DISTINCT o.order_id) as orders_handled,\n",
    "                COUNT(DISTINCT o.customer_id) as customers_served,\n",
    "                COALESCE(ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2), 0) as total_sales,\n",
    "                COALESCE(ROUND(AVG(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2), 0) as avg_order_value,\n",
    "                COALESCE(SUM(od.quantity), 0) as total_units_sold,\n",
    "                COUNT(DISTINCT DATE_PART('year', o.order_date)) as years_active,\n",
    "                MIN(o.order_date) as first_sale_date,\n",
    "                MAX(o.order_date) as last_sale_date\n",
    "            FROM {schema}.employees e\n",
    "            LEFT JOIN {schema}.employees mgr ON e.reports_to = mgr.employee_id\n",
    "            LEFT JOIN {schema}.orders o ON e.employee_id = o.employee_id\n",
    "            LEFT JOIN {schema}.order_details od ON o.order_id = od.order_id\n",
    "            GROUP BY e.employee_id, e.first_name, e.last_name, e.title, e.title_of_courtesy,\n",
    "                     e.birth_date, e.hire_date, e.city, e.region, e.country, e.home_phone,\n",
    "                     e.reports_to, mgr.first_name, mgr.last_name\n",
    "        )\n",
    "        SELECT * FROM employee_performance\n",
    "        ORDER BY total_sales DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_employees = pd.read_sql_query(employee_analysis_query, conn_string)\n",
    "        \n",
    "        employee_doc = \"NORTHWIND EMPLOYEE PERFORMANCE AND ORGANIZATIONAL ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Organizational overview\n",
    "        employee_doc += f\"ORGANIZATIONAL STRUCTURE:\\n\"\n",
    "        employee_doc += f\"Total Employees: {len(df_employees)}\\n\"\n",
    "        \n",
    "        # Management hierarchy\n",
    "        managers = df_employees[df_employees['reports_to'].isna()]\n",
    "        subordinates = df_employees[df_employees['reports_to'].notna()]\n",
    "        \n",
    "        employee_doc += f\"Management Level: {len(managers)} managers\\n\"\n",
    "        employee_doc += f\"Staff Level: {len(subordinates)} employees\\n\\n\"\n",
    "        \n",
    "        # Employee performance ranking\n",
    "        employee_doc += \"EMPLOYEE SALES PERFORMANCE RANKING:\\n\"\n",
    "        for i, row in df_employees.iterrows():\n",
    "            manager_info = f\" (Reports to: {row['manager_name']})\" if pd.notna(row['manager_name']) else \" (Senior Management)\"\n",
    "            years_service = datetime.now().year - pd.to_datetime(row['hire_date']).year if pd.notna(row['hire_date']) else 0\n",
    "            age = datetime.now().year - pd.to_datetime(row['birth_date']).year if pd.notna(row['birth_date']) else 0\n",
    "            \n",
    "            employee_doc += f\"{i+1}. {row['full_name']} - {row['title']}\\n\"\n",
    "            employee_doc += f\"   • Total Sales: ${row['total_sales']:,.2f} across {row['orders_handled']} orders\\n\"\n",
    "            employee_doc += f\"   • Customers Served: {row['customers_served']}, Avg Order Value: ${row['avg_order_value']:,.2f}\\n\"\n",
    "            employee_doc += f\"   • Service Period: {years_service} years (hired {str(row['hire_date'])[:10]})\\n\"\n",
    "            employee_doc += f\"   • Age: {age}, Location: {row['city']}, {row['country']}\\n\"\n",
    "            employee_doc += f\"   • Contact: {row['home_phone']}{manager_info}\\n\\n\"\n",
    "        \n",
    "        # Performance metrics analysis\n",
    "        total_company_sales = df_employees['total_sales'].sum()\n",
    "        top_performer = df_employees.iloc[0]\n",
    "        \n",
    "        employee_doc += \"PERFORMANCE INSIGHTS:\\n\"\n",
    "        employee_doc += f\"• Top Performer: {top_performer['full_name']} (${top_performer['total_sales']:,.2f} - {(top_performer['total_sales']/total_company_sales*100):.1f}% of total sales)\\n\"\n",
    "        employee_doc += f\"• Average Sales per Employee: ${df_employees['total_sales'].mean():,.2f}\\n\"\n",
    "        employee_doc += f\"• Sales Performance Range: ${df_employees['total_sales'].min():,.2f} - ${df_employees['total_sales'].max():,.2f}\\n\"\n",
    "        employee_doc += f\"• Average Customer Base per Employee: {df_employees['customers_served'].mean():.1f} customers\\n\"\n",
    "        \n",
    "        documents.append(employee_doc)\n",
    "        \n",
    "        # 6. SHIPPING AND LOGISTICS ANALYSIS\n",
    "        print(\"Generating shipping and logistics analysis...\")\n",
    "        \n",
    "        shipping_analysis_query = f\"\"\"\n",
    "        WITH shipping_metrics AS (\n",
    "    SELECT \n",
    "        sh.shipper_id,\n",
    "        sh.company_name as shipper_name,\n",
    "        sh.phone as shipper_phone,\n",
    "        COUNT(o.order_id) as total_shipments,\n",
    "        COUNT(DISTINCT o.customer_id) as customers_served,\n",
    "        COUNT(DISTINCT o.ship_country) as countries_served,\n",
    "        ROUND(AVG(o.freight)::numeric, 2) as avg_freight_cost,\n",
    "        ROUND(SUM(o.freight)::numeric, 2) as total_freight_revenue,\n",
    "        ROUND(AVG((o.shipped_date::date - o.order_date::date))::numeric, 1) as avg_delivery_days,\n",
    "        COUNT(CASE WHEN o.shipped_date::date > o.required_date::date THEN 1 END) as late_deliveries,\n",
    "        ROUND((COUNT(CASE WHEN o.shipped_date::date > o.required_date::date THEN 1 END) * 100.0 / COUNT(o.order_id))::numeric, 2) as late_delivery_rate\n",
    "    FROM {schema}.shippers sh\n",
    "    LEFT JOIN {schema}.orders o ON sh.shipper_id = o.ship_via\n",
    "    WHERE o.shipped_date IS NOT NULL AND o.order_date IS NOT NULL\n",
    "    GROUP BY sh.shipper_id, sh.company_name, sh.phone\n",
    "),\n",
    "route_analysis AS (\n",
    "    SELECT \n",
    "        o.ship_country,\n",
    "        o.ship_city,\n",
    "        COUNT(o.order_id) as shipment_count,\n",
    "        ROUND(AVG(o.freight)::numeric, 2) as avg_freight_cost,\n",
    "        COUNT(DISTINCT o.customer_id) as customers_in_location\n",
    "    FROM {schema}.orders o\n",
    "    WHERE o.shipped_date IS NOT NULL\n",
    "    GROUP BY o.ship_country, o.ship_city\n",
    ")\n",
    "SELECT \n",
    "    sm.*,\n",
    "    (SELECT COUNT(*) FROM route_analysis) as total_shipping_locations\n",
    "FROM shipping_metrics sm\n",
    "ORDER BY sm.total_freight_revenue DESC;\n",
    "        \"\"\"\n",
    "        \n",
    "        df_shipping = pd.read_sql_query(shipping_analysis_query, conn_string)\n",
    "        \n",
    "        # Route analysis\n",
    "        route_query = f\"\"\"\n",
    "        SELECT \n",
    "            ship_country,\n",
    "            ship_city,\n",
    "            COUNT(order_id) as shipment_count,\n",
    "            ROUND(AVG(freight)::numeric, 2) as avg_freight_cost,\n",
    "            COUNT(DISTINCT customer_id) as customers_in_location\n",
    "        FROM {schema}.orders\n",
    "        WHERE shipped_date IS NOT NULL\n",
    "        GROUP BY ship_country, ship_city\n",
    "        ORDER BY shipment_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_routes = pd.read_sql_query(route_query, conn_string)\n",
    "        \n",
    "        shipping_doc = \"NORTHWIND SHIPPING AND LOGISTICS PERFORMANCE ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Shipping company performance\n",
    "        shipping_doc += \"SHIPPING PARTNER PERFORMANCE:\\n\"\n",
    "        for _, row in df_shipping.iterrows():\n",
    "            on_time_rate = 100 - row['late_delivery_rate']\n",
    "            \n",
    "            shipping_doc += f\"• {row['shipper_name']} (Phone: {row['shipper_phone']})\\n\"\n",
    "            shipping_doc += f\"  - Total Shipments: {row['total_shipments']:,} to {row['customers_served']} customers\\n\"\n",
    "            shipping_doc += f\"  - Coverage: {row['countries_served']} countries\\n\"\n",
    "            shipping_doc += f\"  - Freight Revenue: ${row['total_freight_revenue']:,.2f} (Avg: ${row['avg_freight_cost']:.2f} per shipment)\\n\"\n",
    "            shipping_doc += f\"  - Delivery Performance: {row['avg_delivery_days']:.1f} days average, {on_time_rate:.1f}% on-time rate\\n\"\n",
    "            shipping_doc += f\"  - Late Deliveries: {row['late_deliveries']} ({row['late_delivery_rate']:.1f}%)\\n\\n\"\n",
    "        \n",
    "        # Geographic shipping analysis\n",
    "        shipping_doc += \"TOP SHIPPING DESTINATIONS:\\n\"\n",
    "        country_routes = df_routes.groupby('ship_country').agg({\n",
    "            'shipment_count': 'sum',\n",
    "            'avg_freight_cost': 'mean',\n",
    "            'customers_in_location': 'sum',\n",
    "            'ship_city': 'count'\n",
    "        }).sort_values('shipment_count', ascending=False)\n",
    "        \n",
    "        for country, row in country_routes.head(15).iterrows():\n",
    "            shipping_doc += f\"• {country}: {row['shipment_count']} shipments to {row['ship_city']} cities\\n\"\n",
    "            shipping_doc += f\"  - {row['customers_in_location']} customers, avg freight: ${row['avg_freight_cost']:.2f}\\n\"\n",
    "        \n",
    "        shipping_doc += \"\\nTOP CITY DESTINATIONS:\\n\"\n",
    "        for _, row in df_routes.head(20).iterrows():\n",
    "            shipping_doc += f\"• {row['ship_city']}, {row['ship_country']}: {row['shipment_count']} shipments\\n\"\n",
    "            shipping_doc += f\"  - {row['customers_in_location']} customers, avg freight: ${row['avg_freight_cost']:.2f}\\n\"\n",
    "        \n",
    "        documents.append(shipping_doc)\n",
    "        \n",
    "        # 7. FINANCIAL PERFORMANCE AND TRENDS ANALYSIS\n",
    "        print(\"Generating comprehensive financial analysis...\")\n",
    "        \n",
    "        financial_analysis_query = f\"\"\"\n",
    "        WITH monthly_performance AS (\n",
    "            SELECT \n",
    "                EXTRACT(year FROM o.order_date) as year,\n",
    "                EXTRACT(month FROM o.order_date) as month,\n",
    "                COUNT(DISTINCT o.order_id) as order_count,\n",
    "                COUNT(DISTINCT o.customer_id) as active_customers,\n",
    "                COUNT(DISTINCT od.product_id) as products_sold,\n",
    "                SUM(od.quantity) as units_sold,\n",
    "                ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as revenue,\n",
    "                ROUND(AVG(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as avg_order_line_value,\n",
    "                ROUND(SUM(o.freight)::numeric, 2) as total_freight,\n",
    "                ROUND(SUM(od.unit_price * od.quantity * od.discount)::numeric, 2) as total_discounts,\n",
    "                ROUND(AVG(od.discount * 100)::numeric, 2) as avg_discount_percentage\n",
    "            FROM {schema}.orders o\n",
    "            JOIN {schema}.order_details od ON o.order_id = od.order_id\n",
    "            WHERE o.order_date IS NOT NULL\n",
    "            GROUP BY EXTRACT(year FROM o.order_date), EXTRACT(month FROM o.order_date)\n",
    "        ),\n",
    "        quarterly_performance AS (\n",
    "            SELECT \n",
    "                year,\n",
    "                CASE \n",
    "                    WHEN month IN (1,2,3) THEN 'Q1'\n",
    "                    WHEN month IN (4,5,6) THEN 'Q2'\n",
    "                    WHEN month IN (7,8,9) THEN 'Q3'\n",
    "                    ELSE 'Q4'\n",
    "                END as quarter,\n",
    "                SUM(order_count) as orders,\n",
    "                SUM(revenue) as quarterly_revenue,\n",
    "                AVG(active_customers) as avg_monthly_customers,\n",
    "                SUM(units_sold) as total_units,\n",
    "                SUM(total_freight) as freight_revenue\n",
    "            FROM monthly_performance\n",
    "            GROUP BY year, CASE \n",
    "                WHEN month IN (1,2,3) THEN 'Q1'\n",
    "                WHEN month IN (4,5,6) THEN 'Q2'\n",
    "                WHEN month IN (7,8,9) THEN 'Q3'\n",
    "                ELSE 'Q4'\n",
    "            END\n",
    "        )\n",
    "        SELECT * FROM monthly_performance\n",
    "        ORDER BY year, month\n",
    "        \"\"\"\n",
    "        \n",
    "        df_financial = pd.read_sql_query(financial_analysis_query, conn_string)\n",
    "        \n",
    "        financial_doc = \"NORTHWIND COMPREHENSIVE FINANCIAL PERFORMANCE ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Overall financial summary\n",
    "        total_revenue = df_financial['revenue'].sum()\n",
    "        total_orders = df_financial['order_count'].sum()\n",
    "        total_discounts = df_financial['total_discounts'].sum()\n",
    "        \n",
    "        financial_doc += f\"FINANCIAL OVERVIEW:\\n\"\n",
    "        financial_doc += f\"• Total Revenue: ${total_revenue:,.2f}\\n\"\n",
    "        financial_doc += f\"• Total Orders: {total_orders:,}\\n\"\n",
    "        financial_doc += f\"• Total Discounts Given: ${total_discounts:,.2f} ({(total_discounts/total_revenue*100):.1f}% of revenue)\\n\"\n",
    "        financial_doc += f\"• Average Order Value: ${(total_revenue/total_orders):,.2f}\\n\"\n",
    "        financial_doc += f\"• Total Units Sold: {df_financial['units_sold'].sum():,}\\n\\n\"\n",
    "        \n",
    "        # Yearly performance\n",
    "        yearly_summary = df_financial.groupby('year').agg({\n",
    "            'revenue': 'sum',\n",
    "            'order_count': 'sum',\n",
    "            'active_customers': 'mean',\n",
    "            'units_sold': 'sum',\n",
    "            'total_freight': 'sum',\n",
    "            'total_discounts': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        financial_doc += \"ANNUAL PERFORMANCE BREAKDOWN:\\n\"\n",
    "        for year, row in yearly_summary.iterrows():\n",
    "            year_growth = \"\"\n",
    "            if year > yearly_summary.index.min():\n",
    "                prev_year_revenue = yearly_summary.loc[year-1, 'revenue']\n",
    "                growth_rate = ((row['revenue'] - prev_year_revenue) / prev_year_revenue * 100)\n",
    "                year_growth = f\" ({growth_rate:+.1f}% vs prior year)\"\n",
    "            \n",
    "            financial_doc += f\"• {int(year)}: ${row['revenue']:,.2f} revenue{year_growth}\\n\"\n",
    "            financial_doc += f\"  - {int(row['order_count']):,} orders from {row['active_customers']:.0f} avg monthly customers\\n\"\n",
    "            financial_doc += f\"  - {int(row['units_sold']):,} units sold, ${row['total_freight']:,.2f} freight revenue\\n\"\n",
    "            financial_doc += f\"  - ${row['total_discounts']:,.2f} in discounts ({(row['total_discounts']/row['revenue']*100):.1f}%)\\n\"\n",
    "        \n",
    "        # Monthly trends analysis\n",
    "        financial_doc += \"\\nMONTHLY PERFORMANCE TRENDS:\\n\"\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        monthly_avg = df_financial.groupby('month').agg({\n",
    "            'revenue': 'mean',\n",
    "            'order_count': 'mean',\n",
    "            'active_customers': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Find best and worst performing months\n",
    "        best_month = monthly_avg['revenue'].idxmax()\n",
    "        worst_month = monthly_avg['revenue'].idxmin()\n",
    "        \n",
    "        financial_doc += f\"• Best Month: {month_names[int(best_month)-1]} (${monthly_avg.loc[best_month, 'revenue']:,.2f} avg revenue)\\n\"\n",
    "        financial_doc += f\"• Weakest Month: {month_names[int(worst_month)-1]} (${monthly_avg.loc[worst_month, 'revenue']:,.2f} avg revenue)\\n\"\n",
    "        financial_doc += f\"• Seasonal Variance: {((monthly_avg['revenue'].max() - monthly_avg['revenue'].min()) / monthly_avg['revenue'].mean() * 100):.1f}%\\n\\n\"\n",
    "        \n",
    "        financial_doc += \"MONTHLY AVERAGE PERFORMANCE:\\n\"\n",
    "        for month, row in monthly_avg.sort_values('revenue', ascending=False).iterrows():\n",
    "            month_name = month_names[int(month)-1]\n",
    "            financial_doc += f\"• {month_name}: ${row['revenue']:,.2f} revenue, {row['order_count']:.0f} orders, {row['active_customers']:.0f} customers\\n\"\n",
    "        \n",
    "        # Discount analysis\n",
    "        financial_doc += f\"\\nDISCOUNT STRATEGY ANALYSIS:\\n\"\n",
    "        financial_doc += f\"• Average Discount Rate: {df_financial['avg_discount_percentage'].mean():.1f}%\\n\"\n",
    "        financial_doc += f\"• Total Discount Impact: ${total_discounts:,.2f} ({(total_discounts/(total_revenue+total_discounts)*100):.1f}% of gross revenue)\\n\"\n",
    "        financial_doc += f\"• Revenue Recovery Ratio: {((total_revenue/total_discounts) if total_discounts > 0 else 0):.1f}:1\\n\"\n",
    "        \n",
    "        documents.append(financial_doc)\n",
    "        \n",
    "        # 8. ADVANCED BUSINESS INTELLIGENCE AND INSIGHTS\n",
    "        print(\"Generating advanced business intelligence insights...\")\n",
    "        \n",
    "        # Customer loyalty and retention analysis\n",
    "        loyalty_analysis_query = f\"\"\"\n",
    "WITH customer_behavior AS (\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.company_name,\n",
    "        c.country,\n",
    "        COUNT(DISTINCT o.order_id) as total_orders,\n",
    "        COUNT(DISTINCT EXTRACT(year FROM o.order_date::date)) as years_active,\n",
    "        COUNT(DISTINCT EXTRACT(month FROM o.order_date::date)) as months_active,\n",
    "        MIN(o.order_date::date) as first_order,\n",
    "        MAX(o.order_date::date) as last_order,\n",
    "        SUM(od.quantity) as total_items,\n",
    "        COUNT(DISTINCT od.product_id) as product_variety,\n",
    "        ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as total_spent,\n",
    "        -- Calculate customer lifetime in days instead\n",
    "        MAX(o.order_date::date) - MIN(o.order_date::date) as customer_lifetime_days\n",
    "    FROM northwind.customers c\n",
    "    JOIN northwind.orders o ON c.customer_id = o.customer_id\n",
    "    JOIN northwind.order_details od ON o.order_id = od.order_id\n",
    "    WHERE o.order_date IS NOT NULL\n",
    "    GROUP BY c.customer_id, c.company_name, c.country\n",
    ")\n",
    "SELECT *,\n",
    "    CASE \n",
    "        WHEN total_orders >= 10 AND years_active >= 2 THEN 'Loyal'\n",
    "        WHEN total_orders >= 5 AND years_active >= 1 THEN 'Regular'\n",
    "        WHEN total_orders >= 3 THEN 'Developing'\n",
    "        ELSE 'New'\n",
    "    END as customer_segment,\n",
    "    -- Calculate average days between orders as total lifetime / (orders - 1)\n",
    "    CASE \n",
    "        WHEN total_orders > 1 THEN ROUND((customer_lifetime_days::numeric / (total_orders - 1)), 1)\n",
    "        ELSE NULL\n",
    "    END as avg_days_between_orders\n",
    "FROM customer_behavior\n",
    "ORDER BY total_spent DESC;\n",
    "        \"\"\"\n",
    "        \n",
    "        df_loyalty = pd.read_sql_query(loyalty_analysis_query, conn_string)\n",
    "        \n",
    "        # Product affinity analysis\n",
    "        affinity_query = f\"\"\"\n",
    "        WITH product_pairs AS (\n",
    "            SELECT \n",
    "                od1.product_id as product_a,\n",
    "                od2.product_id as product_b,\n",
    "                COUNT(*) as co_occurrence\n",
    "            FROM {schema}.order_details od1\n",
    "            JOIN {schema}.order_details od2 ON od1.order_id = od2.order_id\n",
    "            WHERE od1.product_id < od2.product_id\n",
    "            GROUP BY od1.product_id, od2.product_id\n",
    "            HAVING COUNT(*) >= 3\n",
    "        )\n",
    "        SELECT \n",
    "            pp.product_a,\n",
    "            p1.product_name as product_a_name,\n",
    "            pp.product_b,\n",
    "            p2.product_name as product_b_name,\n",
    "            pp.co_occurrence\n",
    "        FROM product_pairs pp\n",
    "        JOIN {schema}.products p1 ON pp.product_a = p1.product_id\n",
    "        JOIN {schema}.products p2 ON pp.product_b = p2.product_id\n",
    "        ORDER BY pp.co_occurrence DESC\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        df_affinity = pd.read_sql_query(affinity_query, conn_string)\n",
    "        \n",
    "        insights_doc = \"NORTHWIND ADVANCED BUSINESS INTELLIGENCE AND STRATEGIC INSIGHTS:\\n\\n\"\n",
    "        \n",
    "        # Customer segmentation analysis\n",
    "        segment_analysis = df_loyalty['customer_segment'].value_counts()\n",
    "        \n",
    "        insights_doc += \"CUSTOMER LOYALTY SEGMENTATION:\\n\"\n",
    "        for segment, count in segment_analysis.items():\n",
    "            segment_customers = df_loyalty[df_loyalty['customer_segment'] == segment]\n",
    "            avg_spend = segment_customers['total_spent'].mean()\n",
    "            avg_orders = segment_customers['total_orders'].mean()\n",
    "            \n",
    "            insights_doc += f\"• {segment} Customers: {count} ({(count/len(df_loyalty)*100):.1f}%)\\n\"\n",
    "            insights_doc += f\"  - Average Spend: ${avg_spend:,.2f}\\n\"\n",
    "            insights_doc += f\"  - Average Orders: {avg_orders:.1f}\\n\"\n",
    "            insights_doc += f\"  - Example: {segment_customers.iloc[0]['company_name']}\\n\"\n",
    "        \n",
    "        # High-value customer analysis\n",
    "        insights_doc += \"\\nHIGH-VALUE CUSTOMER PROFILE:\\n\"\n",
    "        top_customers = df_loyalty.head(10)\n",
    "        insights_doc += f\"• Top 10 customers represent ${top_customers['total_spent'].sum():,.2f} ({(top_customers['total_spent'].sum()/df_loyalty['total_spent'].sum()*100):.1f}% of total revenue)\\n\"\n",
    "        insights_doc += f\"• Average order frequency: {top_customers['avg_days_between_orders'].mean():.0f} days between orders\\n\"\n",
    "        insights_doc += f\"• Product diversity: {top_customers['product_variety'].mean():.0f} different products per customer\\n\"\n",
    "        \n",
    "        insights_doc += \"\\nTOP 10 MOST VALUABLE CUSTOMERS:\\n\"\n",
    "        for i, row in top_customers.iterrows():\n",
    "            customer_tenure = f\"{row['first_order']:.10} to {row['last_order']:.10}\"\n",
    "            insights_doc += f\"{i+1}. {row['company_name']} ({row['country']})\\n\"\n",
    "            insights_doc += f\"   • Total Value: ${row['total_spent']:,.2f} over {row['total_orders']} orders\\n\"\n",
    "            insights_doc += f\"   • Tenure: {customer_tenure} ({row['years_active']} years)\\n\"\n",
    "            insights_doc += f\"   • Behavior: {row['product_variety']} different products, avg {row['avg_days_between_orders']:.0f} days between orders\\n\"\n",
    "            insights_doc += f\"   • Segment: {row['customer_segment']}\\n\\n\"\n",
    "        \n",
    "        # Product affinity insights\n",
    "        insights_doc += \"PRODUCT AFFINITY ANALYSIS (Frequently Bought Together):\\n\"\n",
    "        insights_doc += \"Products commonly purchased together can inform cross-selling strategies:\\n\\n\"\n",
    "        \n",
    "        for i, row in df_affinity.head(15).iterrows():\n",
    "            insights_doc += f\"• {row['product_a_name']} + {row['product_b_name']}\\n\"\n",
    "            insights_doc += f\"  Purchased together in {row['co_occurrence']} orders\\n\"\n",
    "        \n",
    "        # Business recommendations\n",
    "        insights_doc += \"\\nSTRATEGIC BUSINESS RECOMMENDATIONS:\\n\"\n",
    "        \n",
    "        # Revenue concentration analysis\n",
    "        top_20_pct_customers = len(df_loyalty) // 5\n",
    "        top_20_revenue = df_loyalty.head(top_20_pct_customers)['total_spent'].sum()\n",
    "        total_revenue_check = df_loyalty['total_spent'].sum()\n",
    "        \n",
    "        insights_doc += f\"• Revenue Concentration: Top 20% of customers generate ${top_20_revenue:,.2f} ({(top_20_revenue/total_revenue_check*100):.1f}% of revenue)\\n\"\n",
    "        insights_doc += f\"• Customer Retention: Focus on {segment_analysis['Loyal']} loyal customers who drive consistent revenue\\n\"\n",
    "        insights_doc += f\"• Growth Opportunity: {segment_analysis['Developing']} developing customers show potential for increased engagement\\n\"\n",
    "        \n",
    "        # Seasonal insights\n",
    "        peak_months = df_financial.groupby('month')['revenue'].mean().nlargest(3)\n",
    "        insights_doc += f\"• Seasonal Strategy: Peak sales months are {', '.join([month_names[int(m)-1] for m in peak_months.index])}\\n\"\n",
    "        \n",
    "        # Geographic insights\n",
    "        country_performance = df_loyalty.groupby('country').agg({\n",
    "            'total_spent': 'sum',\n",
    "            'customer_id': 'count'\n",
    "        }).sort_values('total_spent', ascending=False)\n",
    "        \n",
    "        top_country = country_performance.index[0]\n",
    "        insights_doc += f\"• Geographic Focus: {top_country} is the top market with ${country_performance.loc[top_country, 'total_spent']:,.2f} from {country_performance.loc[top_country, 'customer_id']} customers\\n\"\n",
    "        \n",
    "        documents.append(insights_doc)\n",
    "        \n",
    "        # 9. OPERATIONAL EFFICIENCY AND INVENTORY INSIGHTS\n",
    "        print(\"Generating operational efficiency analysis...\")\n",
    "        \n",
    "        operational_query = f\"\"\"\n",
    "WITH order_processing AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        o.employee_id,\n",
    "        o.order_date::date,\n",
    "        o.required_date::date,\n",
    "        o.shipped_date::date,\n",
    "        (o.shipped_date::date - o.order_date::date) as processing_days,\n",
    "        (o.required_date::date - o.order_date::date) as promised_delivery_days,\n",
    "        CASE WHEN o.shipped_date::date > o.required_date::date THEN 1 ELSE 0 END as late_delivery,\n",
    "        o.freight,\n",
    "        o.ship_country,\n",
    "        COUNT(od.product_id) as items_in_order,\n",
    "        SUM(od.quantity) as total_quantity,\n",
    "        ROUND(SUM(od.unit_price * od.quantity * (1 - od.discount))::numeric, 2) as order_value\n",
    "    FROM {schema}.orders o\n",
    "    JOIN {schema}.order_details od ON o.order_id = od.order_id\n",
    "    WHERE o.shipped_date IS NOT NULL AND o.order_date IS NOT NULL\n",
    "    GROUP BY o.order_id, o.customer_id, o.employee_id, o.order_date, o.required_date, o.shipped_date, o.freight, o.ship_country\n",
    "),\n",
    "inventory_turnover AS (\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.product_name,\n",
    "        c.category_name,\n",
    "        p.units_in_stock,\n",
    "        p.units_on_order,\n",
    "        p.reorder_level,\n",
    "        COALESCE(SUM(od.quantity), 0) as total_sold,\n",
    "        CASE \n",
    "            WHEN p.units_in_stock > 0 THEN ROUND((COALESCE(SUM(od.quantity), 0)::numeric / p.units_in_stock), 2)\n",
    "            ELSE 0 \n",
    "        END as turnover_ratio\n",
    "    FROM {schema}.products p\n",
    "    JOIN {schema}.categories c ON p.category_id = c.category_id\n",
    "    LEFT JOIN {schema}.order_details od ON p.product_id = od.product_id\n",
    "    WHERE p.discontinued = 0\n",
    "    GROUP BY p.product_id, p.product_name, c.category_name, p.units_in_stock, p.units_on_order, p.reorder_level\n",
    ")\n",
    "SELECT \n",
    "    'processing' as analysis_type,\n",
    "    ROUND(AVG(processing_days)::numeric, 1) as avg_processing_days,\n",
    "    ROUND(AVG(promised_delivery_days)::numeric, 1) as avg_promised_days,\n",
    "    SUM(late_delivery) as total_late_deliveries,\n",
    "    COUNT(*) as total_orders,\n",
    "    ROUND(AVG(freight)::numeric, 2) as avg_freight_cost\n",
    "FROM order_processing\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'inventory' as analysis_type,\n",
    "    ROUND(AVG(turnover_ratio)::numeric, 2) as avg_turnover,\n",
    "    COUNT(CASE WHEN units_in_stock <= reorder_level THEN 1 END)::numeric as low_stock_items,\n",
    "    COUNT(*)::numeric as total_active_products,\n",
    "    SUM(units_in_stock)::numeric as total_inventory_units,\n",
    "    ROUND(AVG(units_in_stock)::numeric, 1) as avg_stock_per_product\n",
    "FROM inventory_turnover; \"\"\"\n",
    "\n",
    "        df_operational = pd.read_sql_query(operational_query, conn_string)\n",
    "        \n",
    "        \n",
    "        # Detailed inventory analysis\n",
    "        inventory_detail_query = f\"\"\"\n",
    "        SELECT \n",
    "            p.product_name,\n",
    "            c.category_name,\n",
    "            p.units_in_stock,\n",
    "            p.reorder_level,\n",
    "            p.units_on_order,\n",
    "            COALESCE(SUM(od.quantity), 0) as units_sold,\n",
    "            CASE \n",
    "                WHEN p.units_in_stock > 0 AND COALESCE(SUM(od.quantity), 0) > 0\n",
    "                THEN ROUND((COALESCE(SUM(od.quantity), 0) / p.units_in_stock)::numeric, 2)\n",
    "                ELSE 0 \n",
    "            END as turnover_ratio,\n",
    "            CASE \n",
    "                WHEN p.units_in_stock <= p.reorder_level THEN 'LOW STOCK'\n",
    "                WHEN p.units_in_stock = 0 THEN 'OUT OF STOCK'\n",
    "                WHEN p.units_in_stock > p.reorder_level * 3 THEN 'OVERSTOCK'\n",
    "                ELSE 'NORMAL'\n",
    "            END as stock_status\n",
    "        FROM {schema}.products p\n",
    "        JOIN {schema}.categories c ON p.category_id = c.category_id\n",
    "        LEFT JOIN {schema}.order_details od ON p.product_id = od.product_id\n",
    "        WHERE p.discontinued = 0\n",
    "        GROUP BY p.product_id, p.product_name, c.category_name, \n",
    "                 p.units_in_stock, p.reorder_level, p.units_on_order\n",
    "        ORDER BY turnover_ratio DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        df_inventory_detail = pd.read_sql_query(inventory_detail_query, conn_string)\n",
    "        \n",
    "        operational_doc = \"NORTHWIND OPERATIONAL EFFICIENCY AND INVENTORY MANAGEMENT ANALYSIS:\\n\\n\"\n",
    "        \n",
    "        # Extract operational metrics\n",
    "        processing_metrics = df_operational[df_operational['analysis_type'] == 'processing'].iloc[0]\n",
    "        inventory_metrics = df_operational[df_operational['analysis_type'] == 'inventory'].iloc[0]\n",
    "\n",
    "        operational_doc += \"ORDER PROCESSING EFFICIENCY:\\n\"\n",
    "        operational_doc += f\"• Average Processing Time: {processing_metrics['avg_processing_days']:.1f} days\\n\"\n",
    "        operational_doc += f\"• Average Promised Delivery: {processing_metrics['avg_promised_days']:.1f} days\\n\"\n",
    "        operational_doc += f\"• Late Deliveries: {processing_metrics['total_late_deliveries']:.0f} out of {processing_metrics['total_orders']:.0f} orders\\n\"\n",
    "        operational_doc += f\"• Average Freight Cost: ${processing_metrics['avg_freight_cost']:.2f} per shipment\\n\\n\"\n",
    "\n",
    "        operational_doc += \"INVENTORY MANAGEMENT PERFORMANCE:\\n\"\n",
    "        operational_doc += f\"• Average Inventory Turnover Ratio: {inventory_metrics['avg_turnover']:.2f}\\n\"\n",
    "        operational_doc += f\"• Products Below Reorder Level: {inventory_metrics['low_stock_items']:.0f} out of {inventory_metrics['total_active_products']:.0f}\\n\"\n",
    "        operational_doc += f\"• Total Inventory Units: {inventory_metrics['total_inventory_units']:.0f}\\n\"\n",
    "        operational_doc += f\"• Average Stock per Product: {inventory_metrics['avg_stock_per_product']:.0f} units\\n\\n\"        \n",
    "        # Stock status analysis\n",
    "        stock_status_summary = df_inventory_detail['stock_status'].value_counts()\n",
    "        \n",
    "        operational_doc += \"INVENTORY STATUS BREAKDOWN:\\n\"\n",
    "        for status, count in stock_status_summary.items():\n",
    "            operational_doc += f\"• {status}: {count} products ({(count/len(df_inventory_detail)*100):.1f}%)\\n\"\n",
    "        \n",
    "        # Critical inventory alerts\n",
    "        critical_items = df_inventory_detail[df_inventory_detail['stock_status'].isin(['LOW STOCK', 'OUT OF STOCK'])]\n",
    "        high_turnover_critical = critical_items[critical_items['turnover_ratio'] > inventory_metrics['avg_turnover']]\n",
    "        \n",
    "        operational_doc += f\"\\nCRITICAL INVENTORY ALERTS:\\n\"\n",
    "        operational_doc += f\"• Items Needing Immediate Attention: {len(critical_items)}\\n\"\n",
    "        operational_doc += f\"• High-Demand Items with Low Stock: {len(high_turnover_critical)}\\n\"\n",
    "        \n",
    "        if len(high_turnover_critical) > 0:\n",
    "            operational_doc += \"\\nURGENT REORDER RECOMMENDATIONS:\\n\"\n",
    "            for _, item in high_turnover_critical.head(10).iterrows():\n",
    "                operational_doc += f\"• {item['product_name']} ({item['category_name']})\\n\"\n",
    "                operational_doc += f\"  - Current Stock: {item['units_in_stock']}, Reorder Level: {item['reorder_level']}\\n\"\n",
    "                operational_doc += f\"  - Turnover Ratio: {item['turnover_ratio']:.2f}, On Order: {item['units_on_order']}\\n\"\n",
    "        \n",
    "        # Best performing inventory\n",
    "        operational_doc += \"\\nTOP PERFORMING PRODUCTS (by turnover):\\n\"\n",
    "        top_performers = df_inventory_detail[df_inventory_detail['turnover_ratio'] > 0].head(15)\n",
    "        for _, item in top_performers.iterrows():\n",
    "            operational_doc += f\"• {item['product_name']}: {item['turnover_ratio']:.2f} turnover ratio\\n\"\n",
    "            operational_doc += f\"  - Stock: {item['units_in_stock']}, Sold: {item['units_sold']}, Status: {item['stock_status']}\\n\"\n",
    "        \n",
    "        documents.append(operational_doc)\n",
    "        \n",
    "        print(f\"Successfully created {len(documents)} comprehensive business documents from Northwind PostgreSQL database\")\n",
    "        print(f\"Total document length: {sum(len(doc) for doc in documents):,} characters\")\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating comprehensive documents: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# # Usage example:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Neon PostgreSQL connection parameters\n",
    "#     business_docs = create_comprehensive_northwind_business_documents(\n",
    "#         host=\"ep-xxx-xxx.us-east-1.aws.neon.tech\",  # Your Neon host\n",
    "#         username=\"your_username\",\n",
    "#         password=\"your_password\",\n",
    "#         database=\"neondb\",\n",
    "#         port=5432,\n",
    "#         schema=\"northwind\"\n",
    "#     )\n",
    "    \n",
    "#     # Preview the documents\n",
    "#     for i, doc in enumerate(business_docs):\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"DOCUMENT {i+1}: {['CUSTOMER ANALYSIS', 'CUSTOMER BEHAVIOR', 'PRODUCT CATALOG', 'SUPPLIER ANALYSIS', 'EMPLOYEE PERFORMANCE', 'SHIPPING LOGISTICS', 'FINANCIAL PERFORMANCE', 'BUSINESS INTELLIGENCE', 'OPERATIONAL EFFICIENCY'][i]}\")\n",
    "#         print('='*80)\n",
    "#         print(doc[:1000] + \"...\" if len(doc) > 1000 else doc)\n",
    "    \n",
    "    # Integration with vector store\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # Convert to LangChain documents with detailed metadata\n",
    "    langchain_docs = []\n",
    "    doc_types = [\n",
    "        \"customer_analysis\", \"customer_behavior\", \"product_catalog\", \n",
    "        \"supplier_analysis\", \"employee_performance\", \"shipping_logistics\",\n",
    "        \"financial_performance\", \"business_intelligence\", \"operational_efficiency\"\n",
    "    ]\n",
    "    \n",
    "    for i, doc in enumerate(business_docs):\n",
    "        langchain_docs.append(Document(\n",
    "            page_content=doc,\n",
    "            metadata={\n",
    "                \"source\": f\"northwind_comprehensive_{doc_types[i]}\",\n",
    "                \"type\": \"business_analysis\",\n",
    "                \"document_id\": i,\n",
    "                \"comprehensive\": True,\n",
    "                \"data_source\": \"postgresql_neon\"\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # Enhanced text splitting for comprehensive documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=600,  # Larger chunks for comprehensive content\n",
    "        chunk_overlap=100,  # More overlap for context preservation\n",
    "        length_function=tiktoken_len,\n",
    "    )\n",
    "    \n",
    "    split_chunks = text_splitter.split_documents(langchain_docs)\n",
    "    \n",
    "    # Create vector store\n",
    "    qdrant_vectorstore = Qdrant.from_documents(\n",
    "        split_chunks,\n",
    "        embedding_model,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"northwind_comprehensive_business_data\",\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Northwind database and generating comprehensive business documents...\n",
      "Generating customer analysis document...\n",
      "Generating customer purchasing behavior analysis...\n",
      "Generating comprehensive product analysis...\n",
      "Generating supplier analysis...\n",
      "Generating employee and territory analysis...\n",
      "Generating shipping and logistics analysis...\n",
      "Generating comprehensive financial analysis...\n",
      "Generating advanced business intelligence insights...\n",
      "Generating operational efficiency analysis...\n",
      "Error creating comprehensive documents: 'avg_turnover'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/viveknatan/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'avg_turnover'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9t/0z36p5hx6dd513bp6v3wxsz80000gn/T/ipykernel_54524/3253639268.py\", line 865, in create_comprehensive_northwind_business_documents\n",
      "    operational_doc += f\"• Average Inventory Turnover Ratio: {inventory_metrics['avg_turnover']:.2f}\\n\"\n",
      "                                                              ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/viveknatan/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/pandas/core/series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "           ~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/viveknatan/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/pandas/core/series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/Users/viveknatan/Documents/AIMCourse/AIDA/New_Version_20250531/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'avg_turnover'\n"
     ]
    }
   ],
   "source": [
    "# # Required parameters only\n",
    "# business_docs = create_northwind_business_documents(\n",
    "#     host=\"your-neon-host.aws.neon.tech\",\n",
    "#     username=\"your_username\", \n",
    "#     password=\"your_password\"\n",
    "# )\n",
    "\n",
    "# # Or with custom database/schema\n",
    "business_docs = create_comprehensive_northwind_business_documents(\n",
    "    host=\"ep-aged-leaf-a5sdyft6-pooler.us-east-2.aws.neon.tech\",\n",
    "    username=\"neondb_owner\",\n",
    "    password=\"npg_m5bUF7retyMH\",\n",
    "    database=\"neondb\",\n",
    "    schema=\"northwind\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context describes the Northwind sample database, which contains 14 interrelated tables relevant to a fictitious trading business, including data on orders, customers, and products. Specifically, the Orders table captures sales orders placed by customers, indicating a potential source for identifying customer activity and sales. However, the context does not include specific data or metrics regarding the top customers in the US or any sales figures that could be leveraged to ascertain customer rankings. Additionally, it references the CustomerDemographics table without providing details about its content. Overall, while the Northwind database features comprehensive customer and order information that could lead to identifying top customers, the exact details of those customers are not presented in the context provided.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_chain_summary.invoke({\"question\" : \"Who are the top customers of Northwind in the US?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context describes the Northwind sample database, which contains 14 interrelated tables relevant to a fictitious trading business, including data on orders, customers, and products. Specifically, the Orders table captures sales orders placed by customers, indicating a potential source for identifying customer activity and sales. However, the context does not include specific data or metrics regarding the top customers in the US or any sales figures that could be leveraged to ascertain customer rankings. Additionally, it references the CustomerDemographics table without providing details about its content. Overall, while the Northwind database features comprehensive customer and order information that could lead to identifying top customers, the exact details of those customers are not presented in the context provided.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_chain_summary.invoke({\"question\" : \"Who are the top customers of Northwind in the US?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context describes the Northwind sample database, which contains 14 interrelated tables relevant to a fictitious trading business, including data on orders, customers, and products. Specifically, the Orders table captures sales orders placed by customers, indicating a potential source for identifying customer activity and sales. However, the context does not include specific data or metrics regarding the top customers in the US or any sales figures that could be leveraged to ascertain customer rankings. Additionally, it references the CustomerDemographics table without providing details about its content. Overall, while the Northwind database features comprehensive customer and order information that could lead to identifying top customers, the exact details of those customers are not presented in the context provided.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_chain_summary.invoke({\"question\" : \"Who are the top customers of Northwind in the US?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
